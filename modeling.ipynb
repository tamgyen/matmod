{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearRegressor:\n",
      "MeanSquaredError: 53678.70\n",
      "RootMeanSquaredError: 231.69\n",
      "MeanAbsoluteError: 89.27\n",
      "FitPercent: 3.23\n",
      "CosineSimilarity: 0.42\n",
      "\n",
      "LassoRegressor:\n",
      "MeanSquaredError: 53678.70\n",
      "RootMeanSquaredError: 231.69\n",
      "MeanAbsoluteError: 89.27\n",
      "FitPercent: 3.23\n",
      "CosineSimilarity: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Anaconda\\envs\\elte_ai\\lib\\site-packages\\pygam\\utils.py:78: UserWarning: Could not import Scikit-Sparse or Suite-Sparse.\n",
      "This will slow down optimization for models with monotonicity/convexity penalties and many splines.\n",
      "See installation instructions for installing Scikit-Sparse and Suite-Sparse via Conda.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GAM:\n",
      "MeanSquaredError: 51597.84\n",
      "RootMeanSquaredError: 227.15\n",
      "MeanAbsoluteError: 90.81\n",
      "FitPercent: 5.12\n",
      "CosineSimilarity: 0.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'MeanSquaredError': 51597.83803796,\n 'RootMeanSquaredError': 227.15157503,\n 'MeanAbsoluteError': 90.8070562,\n 'FitPercent': 5.12093203,\n 'CosineSimilarity': 0.46077381}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import *\n",
    "\n",
    "targets = ['playtime']\n",
    "features = ['m60_rate',\n",
    "            'katana_rate',\n",
    "            'machete_rate',\n",
    "            'pistol_usage',\n",
    "            'chainsaw_usage',\n",
    "            'katana_usage',\n",
    "            'molotov_rate',\n",
    "            'pipe_bomb_rate',\n",
    "            'bile_jar_rate',\n",
    "            'average_rate_utility']\n",
    "\n",
    "df = pd.read_parquet('C:/Dev/ELTE_AI/mat_mod/data/l4d2_player_stats_final_cleaned_normalized.parquet')\n",
    "\n",
    "with open('C:/Dev/ELTE_AI/mat_mod/data/denorm.pkl', 'rb') as file:\n",
    "    denorm = pickle.load(file)\n",
    "\n",
    "df_train, df_test = split_data(df, .75)\n",
    "\n",
    "model = LinearRegressor()\n",
    "results = model.train_and_test(training_data=df_train,\n",
    "                               testing_data=df_test,\n",
    "                               features=features,\n",
    "                               targets=targets,\n",
    "                               denorm=denorm)\n",
    "model = LassoRegressor()\n",
    "model.train_and_test(training_data=df_train,\n",
    "                     features=features,\n",
    "                     testing_data=df_test,\n",
    "                     targets=targets,\n",
    "                     denorm=denorm)\n",
    "model = GAM()\n",
    "model.train_and_test(training_data=df_train,\n",
    "                     features=features,\n",
    "                     testing_data=df_test,\n",
    "                     targets=targets,\n",
    "                     denorm=denorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 11916\n",
      "test samples: 2979\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1408      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,193\n",
      "Trainable params: 10,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1748 - denormalized_mae: 192.5736 \n",
      "Epoch 1: val_loss improved from inf to 0.14552, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 1s 26ms/step - loss: 0.1748 - denormalized_mae: 192.5736 - val_loss: 0.1455 - val_denormalized_mae: 170.7134\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1556 - denormalized_mae: 179.4070\n",
      "Epoch 2: val_loss improved from 0.14552 to 0.14533, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1556 - denormalized_mae: 179.4070 - val_loss: 0.1453 - val_denormalized_mae: 174.0344\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1528 - denormalized_mae: 178.2265\n",
      "Epoch 3: val_loss improved from 0.14533 to 0.14367, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1528 - denormalized_mae: 178.2265 - val_loss: 0.1437 - val_denormalized_mae: 172.3138\n",
      "Epoch 4/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1647 - denormalized_mae: 179.8182\n",
      "Epoch 4: val_loss improved from 0.14367 to 0.13996, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1504 - denormalized_mae: 176.2994 - val_loss: 0.1400 - val_denormalized_mae: 168.9129\n",
      "Epoch 5/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1255 - denormalized_mae: 168.9181\n",
      "Epoch 5: val_loss improved from 0.13996 to 0.13742, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1475 - denormalized_mae: 174.1112 - val_loss: 0.1374 - val_denormalized_mae: 167.8140\n",
      "Epoch 6/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1414 - denormalized_mae: 173.4034\n",
      "Epoch 6: val_loss improved from 0.13742 to 0.13620, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1462 - denormalized_mae: 173.4606 - val_loss: 0.1362 - val_denormalized_mae: 167.3802\n",
      "Epoch 7/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1270 - denormalized_mae: 168.3972\n",
      "Epoch 7: val_loss improved from 0.13620 to 0.13529, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1454 - denormalized_mae: 172.3407 - val_loss: 0.1353 - val_denormalized_mae: 166.0972\n",
      "Epoch 8/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1427 - denormalized_mae: 170.6903\n",
      "Epoch 8: val_loss improved from 0.13529 to 0.13486, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1439 - denormalized_mae: 170.7914 - val_loss: 0.1349 - val_denormalized_mae: 165.7636\n",
      "Epoch 9/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1903 - denormalized_mae: 181.0779\n",
      "Epoch 9: val_loss improved from 0.13486 to 0.13411, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1431 - denormalized_mae: 170.3084 - val_loss: 0.1341 - val_denormalized_mae: 165.5165\n",
      "Epoch 10/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1658 - denormalized_mae: 176.7048\n",
      "Epoch 10: val_loss improved from 0.13411 to 0.13349, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1421 - denormalized_mae: 169.3990 - val_loss: 0.1335 - val_denormalized_mae: 164.5639\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1421 - denormalized_mae: 169.1723\n",
      "Epoch 11: val_loss improved from 0.13349 to 0.13322, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1421 - denormalized_mae: 169.1723 - val_loss: 0.1332 - val_denormalized_mae: 165.0986\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1405 - denormalized_mae: 168.0967\n",
      "Epoch 12: val_loss improved from 0.13322 to 0.13275, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1405 - denormalized_mae: 168.0967 - val_loss: 0.1328 - val_denormalized_mae: 164.6577\n",
      "Epoch 13/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1433 - denormalized_mae: 168.4787\n",
      "Epoch 13: val_loss improved from 0.13275 to 0.13216, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1399 - denormalized_mae: 167.6383 - val_loss: 0.1322 - val_denormalized_mae: 164.1560\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1405 - denormalized_mae: 167.7710\n",
      "Epoch 14: val_loss improved from 0.13216 to 0.13194, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1405 - denormalized_mae: 167.7710 - val_loss: 0.1319 - val_denormalized_mae: 163.6921\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1385 - denormalized_mae: 166.2929\n",
      "Epoch 15: val_loss improved from 0.13194 to 0.13145, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1385 - denormalized_mae: 166.2929 - val_loss: 0.1315 - val_denormalized_mae: 163.6992\n",
      "Epoch 16/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1400 - denormalized_mae: 167.0584\n",
      "Epoch 16: val_loss did not improve from 0.13145\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1389 - denormalized_mae: 166.5408 - val_loss: 0.1315 - val_denormalized_mae: 163.3818\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1384 - denormalized_mae: 166.2471\n",
      "Epoch 17: val_loss improved from 0.13145 to 0.13100, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1384 - denormalized_mae: 166.2471 - val_loss: 0.1310 - val_denormalized_mae: 163.8065\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1385 - denormalized_mae: 166.3556\n",
      "Epoch 18: val_loss did not improve from 0.13100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1385 - denormalized_mae: 166.3556 - val_loss: 0.1310 - val_denormalized_mae: 163.4386\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1382 - denormalized_mae: 166.0175\n",
      "Epoch 19: val_loss improved from 0.13100 to 0.13088, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1382 - denormalized_mae: 166.0175 - val_loss: 0.1309 - val_denormalized_mae: 163.3812\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1382 - denormalized_mae: 165.8483\n",
      "Epoch 20: val_loss improved from 0.13088 to 0.13080, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1382 - denormalized_mae: 165.8483 - val_loss: 0.1308 - val_denormalized_mae: 163.5199\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1375 - denormalized_mae: 165.6896\n",
      "Epoch 21: val_loss improved from 0.13080 to 0.13045, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1375 - denormalized_mae: 165.6896 - val_loss: 0.1305 - val_denormalized_mae: 163.3508\n",
      "Epoch 22/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1807 - denormalized_mae: 179.6683\n",
      "Epoch 22: val_loss did not improve from 0.13045\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1365 - denormalized_mae: 165.1284 - val_loss: 0.1305 - val_denormalized_mae: 162.8934\n",
      "Epoch 23/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1336 - denormalized_mae: 164.0909\n",
      "Epoch 23: val_loss did not improve from 0.13045\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1377 - denormalized_mae: 165.0552 - val_loss: 0.1305 - val_denormalized_mae: 162.8939\n",
      "Epoch 24/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1370 - denormalized_mae: 165.0635\n",
      "Epoch 24: val_loss improved from 0.13045 to 0.13011, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1363 - denormalized_mae: 164.8483 - val_loss: 0.1301 - val_denormalized_mae: 163.4347\n",
      "Epoch 25/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1092 - denormalized_mae: 158.6242\n",
      "Epoch 25: val_loss improved from 0.13011 to 0.12984, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1363 - denormalized_mae: 165.3216 - val_loss: 0.1298 - val_denormalized_mae: 162.7539\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1360 - denormalized_mae: 164.6386\n",
      "Epoch 26: val_loss improved from 0.12984 to 0.12957, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1360 - denormalized_mae: 164.6386 - val_loss: 0.1296 - val_denormalized_mae: 162.5178\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1354 - denormalized_mae: 163.7769\n",
      "Epoch 27: val_loss improved from 0.12957 to 0.12931, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1354 - denormalized_mae: 163.7769 - val_loss: 0.1293 - val_denormalized_mae: 162.3255\n",
      "Epoch 28/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1314 - denormalized_mae: 163.5284\n",
      "Epoch 28: val_loss improved from 0.12931 to 0.12899, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1357 - denormalized_mae: 164.7180 - val_loss: 0.1290 - val_denormalized_mae: 162.5659\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1343 - denormalized_mae: 163.9113\n",
      "Epoch 29: val_loss improved from 0.12899 to 0.12895, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1343 - denormalized_mae: 163.9113 - val_loss: 0.1289 - val_denormalized_mae: 161.9688\n",
      "Epoch 30/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1777 - denormalized_mae: 176.1836\n",
      "Epoch 30: val_loss improved from 0.12895 to 0.12880, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1351 - denormalized_mae: 164.1317 - val_loss: 0.1288 - val_denormalized_mae: 161.8583\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1360 - denormalized_mae: 164.1893\n",
      "Epoch 31: val_loss did not improve from 0.12880\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1360 - denormalized_mae: 164.1893 - val_loss: 0.1291 - val_denormalized_mae: 162.0192\n",
      "Epoch 32/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1686 - denormalized_mae: 171.5643\n",
      "Epoch 32: val_loss improved from 0.12880 to 0.12856, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1343 - denormalized_mae: 164.2384 - val_loss: 0.1286 - val_denormalized_mae: 162.4869\n",
      "Epoch 33/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0985 - denormalized_mae: 155.0488\n",
      "Epoch 33: val_loss improved from 0.12856 to 0.12840, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1337 - denormalized_mae: 163.4573 - val_loss: 0.1284 - val_denormalized_mae: 161.6981\n",
      "Epoch 34/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1059 - denormalized_mae: 157.6170\n",
      "Epoch 34: val_loss improved from 0.12840 to 0.12839, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1344 - denormalized_mae: 163.7879 - val_loss: 0.1284 - val_denormalized_mae: 162.3054\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1345 - denormalized_mae: 163.7887\n",
      "Epoch 35: val_loss did not improve from 0.12839\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1345 - denormalized_mae: 163.7887 - val_loss: 0.1284 - val_denormalized_mae: 161.8863\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1342 - denormalized_mae: 163.7981\n",
      "Epoch 36: val_loss improved from 0.12839 to 0.12812, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.1342 - denormalized_mae: 163.7981 - val_loss: 0.1281 - val_denormalized_mae: 161.9368\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1342 - denormalized_mae: 163.4853\n",
      "Epoch 37: val_loss did not improve from 0.12812\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1342 - denormalized_mae: 163.4853 - val_loss: 0.1283 - val_denormalized_mae: 161.6276\n",
      "Epoch 38/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1406 - denormalized_mae: 164.4403\n",
      "Epoch 38: val_loss improved from 0.12812 to 0.12776, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1337 - denormalized_mae: 163.7437 - val_loss: 0.1278 - val_denormalized_mae: 161.7603\n",
      "Epoch 39/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1580 - denormalized_mae: 170.4579\n",
      "Epoch 39: val_loss improved from 0.12776 to 0.12775, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1333 - denormalized_mae: 163.2893 - val_loss: 0.1277 - val_denormalized_mae: 161.3141\n",
      "Epoch 40/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1081 - denormalized_mae: 155.8743\n",
      "Epoch 40: val_loss improved from 0.12775 to 0.12758, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1333 - denormalized_mae: 163.0380 - val_loss: 0.1276 - val_denormalized_mae: 161.8187\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1327 - denormalized_mae: 163.1559\n",
      "Epoch 41: val_loss improved from 0.12758 to 0.12744, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1327 - denormalized_mae: 163.1559 - val_loss: 0.1274 - val_denormalized_mae: 161.3946\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1332 - denormalized_mae: 163.0917\n",
      "Epoch 42: val_loss did not improve from 0.12744\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1332 - denormalized_mae: 163.0917 - val_loss: 0.1276 - val_denormalized_mae: 161.1889\n",
      "Epoch 43/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1036 - denormalized_mae: 154.8860\n",
      "Epoch 43: val_loss improved from 0.12744 to 0.12733, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1325 - denormalized_mae: 163.1612 - val_loss: 0.1273 - val_denormalized_mae: 161.7110\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1322 - denormalized_mae: 162.9042\n",
      "Epoch 44: val_loss improved from 0.12733 to 0.12721, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1322 - denormalized_mae: 162.9042 - val_loss: 0.1272 - val_denormalized_mae: 161.1544\n",
      "Epoch 45/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1216 - denormalized_mae: 159.7744\n",
      "Epoch 45: val_loss did not improve from 0.12721\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1319 - denormalized_mae: 162.2640 - val_loss: 0.1272 - val_denormalized_mae: 160.9865\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1321 - denormalized_mae: 162.5908\n",
      "Epoch 46: val_loss improved from 0.12721 to 0.12693, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1321 - denormalized_mae: 162.5908 - val_loss: 0.1269 - val_denormalized_mae: 161.6793\n",
      "Epoch 47/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1771 - denormalized_mae: 176.0578\n",
      "Epoch 47: val_loss did not improve from 0.12693\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1327 - denormalized_mae: 162.9290 - val_loss: 0.1270 - val_denormalized_mae: 160.6842\n",
      "Epoch 48/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1407 - denormalized_mae: 162.7439\n",
      "Epoch 48: val_loss improved from 0.12693 to 0.12653, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1314 - denormalized_mae: 162.2892 - val_loss: 0.1265 - val_denormalized_mae: 161.0100\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1314 - denormalized_mae: 162.4494\n",
      "Epoch 49: val_loss did not improve from 0.12653\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1314 - denormalized_mae: 162.4494 - val_loss: 0.1266 - val_denormalized_mae: 160.9063\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1321 - denormalized_mae: 162.6821\n",
      "Epoch 50: val_loss improved from 0.12653 to 0.12641, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1321 - denormalized_mae: 162.6821 - val_loss: 0.1264 - val_denormalized_mae: 161.1529\n",
      "Epoch 51/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1339 - denormalized_mae: 165.1559\n",
      "Epoch 51: val_loss improved from 0.12641 to 0.12639, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1319 - denormalized_mae: 162.9669 - val_loss: 0.1264 - val_denormalized_mae: 160.8367\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1306 - denormalized_mae: 162.0864\n",
      "Epoch 52: val_loss did not improve from 0.12639\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1306 - denormalized_mae: 162.0864 - val_loss: 0.1264 - val_denormalized_mae: 160.8820\n",
      "Epoch 53/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1340 - denormalized_mae: 163.2371\n",
      "Epoch 53: val_loss improved from 0.12639 to 0.12595, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.1315 - denormalized_mae: 162.6042 - val_loss: 0.1259 - val_denormalized_mae: 160.5486\n",
      "Epoch 54/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1261 - denormalized_mae: 161.8226\n",
      "Epoch 54: val_loss did not improve from 0.12595\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1314 - denormalized_mae: 161.9078 - val_loss: 0.1264 - val_denormalized_mae: 160.6158\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1312 - denormalized_mae: 162.6436\n",
      "Epoch 55: val_loss improved from 0.12595 to 0.12590, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1312 - denormalized_mae: 162.6436 - val_loss: 0.1259 - val_denormalized_mae: 160.7155\n",
      "Epoch 56/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1637 - denormalized_mae: 171.0880\n",
      "Epoch 56: val_loss improved from 0.12590 to 0.12580, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1313 - denormalized_mae: 161.9534 - val_loss: 0.1258 - val_denormalized_mae: 160.2176\n",
      "Epoch 57/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1220 - denormalized_mae: 159.1194\n",
      "Epoch 57: val_loss did not improve from 0.12580\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1318 - denormalized_mae: 162.3675 - val_loss: 0.1260 - val_denormalized_mae: 161.0170\n",
      "Epoch 58/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1335 - denormalized_mae: 162.5447\n",
      "Epoch 58: val_loss improved from 0.12580 to 0.12569, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1302 - denormalized_mae: 161.6382 - val_loss: 0.1257 - val_denormalized_mae: 160.4939\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1305 - denormalized_mae: 161.8886\n",
      "Epoch 59: val_loss improved from 0.12569 to 0.12562, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1305 - denormalized_mae: 161.8886 - val_loss: 0.1256 - val_denormalized_mae: 160.4684\n",
      "Epoch 60/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1375 - denormalized_mae: 162.5956\n",
      "Epoch 60: val_loss did not improve from 0.12562\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1303 - denormalized_mae: 161.6954 - val_loss: 0.1257 - val_denormalized_mae: 160.5263\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1295 - denormalized_mae: 161.3852\n",
      "Epoch 61: val_loss did not improve from 0.12562\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1295 - denormalized_mae: 161.3852 - val_loss: 0.1259 - val_denormalized_mae: 160.0754\n",
      "Epoch 62/500\n",
      " 9/12 [=====================>........] - ETA: 0s - loss: 0.1334 - denormalized_mae: 162.4399\n",
      "Epoch 62: val_loss improved from 0.12562 to 0.12561, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1298 - denormalized_mae: 161.6000 - val_loss: 0.1256 - val_denormalized_mae: 160.8910\n",
      "Epoch 63/500\n",
      " 8/12 [===================>..........] - ETA: 0s - loss: 0.1242 - denormalized_mae: 160.1953\n",
      "Epoch 63: val_loss improved from 0.12561 to 0.12524, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.1288 - denormalized_mae: 161.2395 - val_loss: 0.1252 - val_denormalized_mae: 160.1914\n",
      "Epoch 64/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1297 - denormalized_mae: 161.3941\n",
      "Epoch 64: val_loss did not improve from 0.12524\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1297 - denormalized_mae: 161.4496 - val_loss: 0.1254 - val_denormalized_mae: 160.5951\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1302 - denormalized_mae: 162.1254\n",
      "Epoch 65: val_loss improved from 0.12524 to 0.12511, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1302 - denormalized_mae: 162.1254 - val_loss: 0.1251 - val_denormalized_mae: 160.0977\n",
      "Epoch 66/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1063 - denormalized_mae: 154.7020\n",
      "Epoch 66: val_loss did not improve from 0.12511\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1294 - denormalized_mae: 160.8928 - val_loss: 0.1256 - val_denormalized_mae: 160.0879\n",
      "Epoch 67/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1075 - denormalized_mae: 155.7622\n",
      "Epoch 67: val_loss did not improve from 0.12511\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1293 - denormalized_mae: 161.2868 - val_loss: 0.1252 - val_denormalized_mae: 160.4210\n",
      "Epoch 68/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1276 - denormalized_mae: 160.6236\n",
      "Epoch 68: val_loss did not improve from 0.12511\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1295 - denormalized_mae: 161.6044 - val_loss: 0.1252 - val_denormalized_mae: 159.9449\n",
      "Epoch 69/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1670 - denormalized_mae: 172.2350\n",
      "Epoch 69: val_loss did not improve from 0.12511\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1297 - denormalized_mae: 160.8260 - val_loss: 0.1255 - val_denormalized_mae: 160.1811\n",
      "Epoch 70/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1466 - denormalized_mae: 167.2380\n",
      "Epoch 70: val_loss improved from 0.12511 to 0.12468, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1285 - denormalized_mae: 161.1478 - val_loss: 0.1247 - val_denormalized_mae: 160.4427\n",
      "Epoch 71/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0870 - denormalized_mae: 150.2547\n",
      "Epoch 71: val_loss did not improve from 0.12468\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1292 - denormalized_mae: 161.3300 - val_loss: 0.1250 - val_denormalized_mae: 159.7292\n",
      "Epoch 72/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1472 - denormalized_mae: 165.7029\n",
      "Epoch 72: val_loss improved from 0.12468 to 0.12462, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1281 - denormalized_mae: 160.8956 - val_loss: 0.1246 - val_denormalized_mae: 160.6228\n",
      "Epoch 73/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1391 - denormalized_mae: 163.2269\n",
      "Epoch 73: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1301 - denormalized_mae: 161.5824 - val_loss: 0.1252 - val_denormalized_mae: 159.8259\n",
      "Epoch 74/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1680 - denormalized_mae: 169.8171\n",
      "Epoch 74: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1293 - denormalized_mae: 160.9536 - val_loss: 0.1250 - val_denormalized_mae: 160.4501\n",
      "Epoch 75/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1466 - denormalized_mae: 164.2473\n",
      "Epoch 75: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1288 - denormalized_mae: 161.2507 - val_loss: 0.1249 - val_denormalized_mae: 159.9850\n",
      "Epoch 76/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1399 - denormalized_mae: 164.1083\n",
      "Epoch 76: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1285 - denormalized_mae: 160.6477 - val_loss: 0.1250 - val_denormalized_mae: 160.2816\n",
      "Epoch 77/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1361 - denormalized_mae: 162.1919\n",
      "Epoch 77: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1290 - denormalized_mae: 161.4032 - val_loss: 0.1249 - val_denormalized_mae: 159.9931\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1288 - denormalized_mae: 160.1852\n",
      "Epoch 78: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1288 - denormalized_mae: 160.1852 - val_loss: 0.1254 - val_denormalized_mae: 160.0364\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1281 - denormalized_mae: 161.0645\n",
      "Epoch 79: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1281 - denormalized_mae: 161.0645 - val_loss: 0.1252 - val_denormalized_mae: 160.1213\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1277 - denormalized_mae: 160.3013\n",
      "Epoch 80: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1277 - denormalized_mae: 160.3013 - val_loss: 0.1249 - val_denormalized_mae: 159.9889\n",
      "Epoch 81/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1752 - denormalized_mae: 173.1692\n",
      "Epoch 81: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1279 - denormalized_mae: 160.7595 - val_loss: 0.1249 - val_denormalized_mae: 159.9253\n",
      "Epoch 82/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1263 - denormalized_mae: 160.2579\n",
      "Epoch 82: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1280 - denormalized_mae: 160.4381 - val_loss: 0.1252 - val_denormalized_mae: 159.5401\n",
      "Epoch 83/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1585 - denormalized_mae: 167.7638\n",
      "Epoch 83: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1270 - denormalized_mae: 160.1493 - val_loss: 0.1250 - val_denormalized_mae: 160.1689\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1274 - denormalized_mae: 160.3245\n",
      "Epoch 84: val_loss did not improve from 0.12462\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1274 - denormalized_mae: 160.3245 - val_loss: 0.1252 - val_denormalized_mae: 159.7187\n",
      "Epoch 85/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1332 - denormalized_mae: 161.9649\n",
      "Epoch 85: val_loss improved from 0.12462 to 0.12456, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1275 - denormalized_mae: 160.7704 - val_loss: 0.1246 - val_denormalized_mae: 160.0190\n",
      "Epoch 86/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1441 - denormalized_mae: 165.5054\n",
      "Epoch 86: val_loss did not improve from 0.12456\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1288 - denormalized_mae: 160.1468 - val_loss: 0.1256 - val_denormalized_mae: 159.8587\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1279 - denormalized_mae: 160.6098\n",
      "Epoch 87: val_loss did not improve from 0.12456\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1279 - denormalized_mae: 160.6098 - val_loss: 0.1246 - val_denormalized_mae: 160.4236\n",
      "Epoch 88/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.2064 - denormalized_mae: 181.9792\n",
      "Epoch 88: val_loss did not improve from 0.12456\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1274 - denormalized_mae: 160.7886 - val_loss: 0.1249 - val_denormalized_mae: 159.7488\n",
      "Epoch 89/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1579 - denormalized_mae: 167.7209\n",
      "Epoch 89: val_loss did not improve from 0.12456\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1277 - denormalized_mae: 160.2417 - val_loss: 0.1252 - val_denormalized_mae: 159.8248\n",
      "Epoch 90/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1364 - denormalized_mae: 163.9144\n",
      "Epoch 90: val_loss improved from 0.12456 to 0.12451, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1270 - denormalized_mae: 160.3090 - val_loss: 0.1245 - val_denormalized_mae: 160.1228\n",
      "Epoch 91/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1628 - denormalized_mae: 169.6636\n",
      "Epoch 91: val_loss did not improve from 0.12451\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1275 - denormalized_mae: 160.6187 - val_loss: 0.1246 - val_denormalized_mae: 159.6507\n",
      "Epoch 92/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1477 - denormalized_mae: 165.6996\n",
      "Epoch 92: val_loss did not improve from 0.12451\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1268 - denormalized_mae: 159.7309 - val_loss: 0.1249 - val_denormalized_mae: 159.9941\n",
      "Epoch 93/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1485 - denormalized_mae: 166.8495\n",
      "Epoch 93: val_loss improved from 0.12451 to 0.12443, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1276 - denormalized_mae: 160.2643 - val_loss: 0.1244 - val_denormalized_mae: 159.6048\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1274 - denormalized_mae: 160.0319\n",
      "Epoch 94: val_loss did not improve from 0.12443\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1274 - denormalized_mae: 160.0319 - val_loss: 0.1249 - val_denormalized_mae: 160.0714\n",
      "Epoch 95/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1301 - denormalized_mae: 160.2557\n",
      "Epoch 95: val_loss improved from 0.12443 to 0.12422, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1270 - denormalized_mae: 160.4907 - val_loss: 0.1242 - val_denormalized_mae: 159.8386\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1266 - denormalized_mae: 159.9826\n",
      "Epoch 96: val_loss did not improve from 0.12422\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1266 - denormalized_mae: 159.9826 - val_loss: 0.1244 - val_denormalized_mae: 159.6393\n",
      "Epoch 97/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1244 - denormalized_mae: 159.4792\n",
      "Epoch 97: val_loss did not improve from 0.12422\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1278 - denormalized_mae: 160.1592 - val_loss: 0.1244 - val_denormalized_mae: 159.3200\n",
      "Epoch 98/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1319 - denormalized_mae: 161.9776\n",
      "Epoch 98: val_loss improved from 0.12422 to 0.12421, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1271 - denormalized_mae: 160.0360 - val_loss: 0.1242 - val_denormalized_mae: 159.7601\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1272 - denormalized_mae: 160.1300\n",
      "Epoch 99: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1272 - denormalized_mae: 160.1300 - val_loss: 0.1249 - val_denormalized_mae: 160.1502\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1266 - denormalized_mae: 160.0541\n",
      "Epoch 100: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1266 - denormalized_mae: 160.0541 - val_loss: 0.1246 - val_denormalized_mae: 160.0779\n",
      "Epoch 101/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1184 - denormalized_mae: 158.3336\n",
      "Epoch 101: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1269 - denormalized_mae: 160.0812 - val_loss: 0.1248 - val_denormalized_mae: 159.5989\n",
      "Epoch 102/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1132 - denormalized_mae: 155.7758\n",
      "Epoch 102: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1266 - denormalized_mae: 159.8750 - val_loss: 0.1247 - val_denormalized_mae: 159.9273\n",
      "Epoch 103/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0915 - denormalized_mae: 150.5275\n",
      "Epoch 103: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1271 - denormalized_mae: 160.3126 - val_loss: 0.1244 - val_denormalized_mae: 160.0791\n",
      "Epoch 104/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0944 - denormalized_mae: 151.9669\n",
      "Epoch 104: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1269 - denormalized_mae: 160.0871 - val_loss: 0.1244 - val_denormalized_mae: 159.3885\n",
      "Epoch 105/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0819 - denormalized_mae: 147.7332\n",
      "Epoch 105: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1272 - denormalized_mae: 159.7453 - val_loss: 0.1248 - val_denormalized_mae: 159.7201\n",
      "Epoch 106/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1099 - denormalized_mae: 156.0630\n",
      "Epoch 106: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1262 - denormalized_mae: 159.9762 - val_loss: 0.1244 - val_denormalized_mae: 160.0667\n",
      "Epoch 107/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1509 - denormalized_mae: 166.6873\n",
      "Epoch 107: val_loss did not improve from 0.12421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1259 - denormalized_mae: 159.8490 - val_loss: 0.1243 - val_denormalized_mae: 159.7707\n",
      "Epoch 108/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1541 - denormalized_mae: 166.6767\n",
      "Epoch 108: val_loss improved from 0.12421 to 0.12401, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1254 - denormalized_mae: 159.4667 - val_loss: 0.1240 - val_denormalized_mae: 159.5153\n",
      "Epoch 109/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0922 - denormalized_mae: 150.2008\n",
      "Epoch 109: val_loss did not improve from 0.12401\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1267 - denormalized_mae: 159.7611 - val_loss: 0.1247 - val_denormalized_mae: 159.5975\n",
      "Epoch 110/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1065 - denormalized_mae: 154.1361\n",
      "Epoch 110: val_loss did not improve from 0.12401\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1265 - denormalized_mae: 159.7198 - val_loss: 0.1244 - val_denormalized_mae: 159.9284\n",
      "Epoch 111/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1573 - denormalized_mae: 167.8965\n",
      "Epoch 111: val_loss improved from 0.12401 to 0.12398, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1262 - denormalized_mae: 160.4628 - val_loss: 0.1240 - val_denormalized_mae: 159.6068\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1255 - denormalized_mae: 159.0980\n",
      "Epoch 112: val_loss improved from 0.12398 to 0.12382, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1255 - denormalized_mae: 159.0980 - val_loss: 0.1238 - val_denormalized_mae: 159.8822\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1257 - denormalized_mae: 160.1681\n",
      "Epoch 113: val_loss did not improve from 0.12382\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1257 - denormalized_mae: 160.1681 - val_loss: 0.1239 - val_denormalized_mae: 159.2746\n",
      "Epoch 114/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1303 - denormalized_mae: 160.1466\n",
      "Epoch 114: val_loss did not improve from 0.12382\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1263 - denormalized_mae: 159.7085 - val_loss: 0.1241 - val_denormalized_mae: 159.8601\n",
      "Epoch 115/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1241 - denormalized_mae: 158.4058\n",
      "Epoch 115: val_loss improved from 0.12382 to 0.12370, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1254 - denormalized_mae: 159.6677 - val_loss: 0.1237 - val_denormalized_mae: 159.1590\n",
      "Epoch 116/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1164 - denormalized_mae: 156.6046\n",
      "Epoch 116: val_loss did not improve from 0.12370\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1262 - denormalized_mae: 159.4861 - val_loss: 0.1239 - val_denormalized_mae: 159.6764\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1263 - denormalized_mae: 159.9056\n",
      "Epoch 117: val_loss did not improve from 0.12370\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1263 - denormalized_mae: 159.9056 - val_loss: 0.1238 - val_denormalized_mae: 159.5068\n",
      "Epoch 118/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1604 - denormalized_mae: 168.1389\n",
      "Epoch 118: val_loss did not improve from 0.12370\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1259 - denormalized_mae: 159.8123 - val_loss: 0.1240 - val_denormalized_mae: 160.0430\n",
      "Epoch 119/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1247 - denormalized_mae: 159.5101\n",
      "Epoch 119: val_loss improved from 0.12370 to 0.12368, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1259 - denormalized_mae: 160.0346 - val_loss: 0.1237 - val_denormalized_mae: 159.0918\n",
      "Epoch 120/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1717 - denormalized_mae: 171.3149\n",
      "Epoch 120: val_loss did not improve from 0.12368\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1241 - denormalized_mae: 158.8705 - val_loss: 0.1239 - val_denormalized_mae: 159.6749\n",
      "Epoch 121/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1243 - denormalized_mae: 159.5788\n",
      "Epoch 121: val_loss improved from 0.12368 to 0.12365, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1251 - denormalized_mae: 159.7846 - val_loss: 0.1236 - val_denormalized_mae: 159.2701\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1252 - denormalized_mae: 159.4212\n",
      "Epoch 122: val_loss improved from 0.12365 to 0.12360, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1252 - denormalized_mae: 159.4212 - val_loss: 0.1236 - val_denormalized_mae: 159.7435\n",
      "Epoch 123/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1373 - denormalized_mae: 161.2950\n",
      "Epoch 123: val_loss improved from 0.12360 to 0.12312, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1239 - denormalized_mae: 159.2654 - val_loss: 0.1231 - val_denormalized_mae: 159.0061\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1238 - denormalized_mae: 158.7670\n",
      "Epoch 124: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1238 - denormalized_mae: 158.7670 - val_loss: 0.1236 - val_denormalized_mae: 159.3964\n",
      "Epoch 125/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1027 - denormalized_mae: 153.2539\n",
      "Epoch 125: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1238 - denormalized_mae: 159.0059 - val_loss: 0.1238 - val_denormalized_mae: 159.4427\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1241 - denormalized_mae: 159.0653\n",
      "Epoch 126: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1241 - denormalized_mae: 159.0653 - val_loss: 0.1236 - val_denormalized_mae: 159.6981\n",
      "Epoch 127/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1319 - denormalized_mae: 160.7313\n",
      "Epoch 127: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1250 - denormalized_mae: 159.2040 - val_loss: 0.1238 - val_denormalized_mae: 159.1400\n",
      "Epoch 128/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1674 - denormalized_mae: 168.0612\n",
      "Epoch 128: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1249 - denormalized_mae: 159.2708 - val_loss: 0.1240 - val_denormalized_mae: 159.8555\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1254 - denormalized_mae: 159.6546\n",
      "Epoch 129: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1254 - denormalized_mae: 159.6546 - val_loss: 0.1237 - val_denormalized_mae: 159.4182\n",
      "Epoch 130/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1279 - denormalized_mae: 161.7697\n",
      "Epoch 130: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1254 - denormalized_mae: 159.4970 - val_loss: 0.1239 - val_denormalized_mae: 159.9586\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1244 - denormalized_mae: 159.5406\n",
      "Epoch 131: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1244 - denormalized_mae: 159.5406 - val_loss: 0.1233 - val_denormalized_mae: 159.0916\n",
      "Epoch 132/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1002 - denormalized_mae: 152.6825\n",
      "Epoch 132: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1245 - denormalized_mae: 158.9566 - val_loss: 0.1235 - val_denormalized_mae: 159.5968\n",
      "Epoch 133/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1164 - denormalized_mae: 157.5088\n",
      "Epoch 133: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1249 - denormalized_mae: 159.3193 - val_loss: 0.1235 - val_denormalized_mae: 159.1571\n",
      "Epoch 134/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1168 - denormalized_mae: 157.1828\n",
      "Epoch 134: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1248 - denormalized_mae: 159.7183 - val_loss: 0.1236 - val_denormalized_mae: 159.1848\n",
      "Epoch 135/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1237 - denormalized_mae: 159.1334\n",
      "Epoch 135: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1259 - denormalized_mae: 159.5314 - val_loss: 0.1241 - val_denormalized_mae: 159.8924\n",
      "Epoch 136/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0776 - denormalized_mae: 145.6038\n",
      "Epoch 136: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1251 - denormalized_mae: 159.6782 - val_loss: 0.1238 - val_denormalized_mae: 159.3818\n",
      "Epoch 137/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1530 - denormalized_mae: 165.7231\n",
      "Epoch 137: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1254 - denormalized_mae: 158.9450 - val_loss: 0.1242 - val_denormalized_mae: 159.7815\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1238 - denormalized_mae: 159.1617\n",
      "Epoch 138: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1238 - denormalized_mae: 159.1617 - val_loss: 0.1232 - val_denormalized_mae: 159.8259\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1257 - denormalized_mae: 159.6269\n",
      "Epoch 139: val_loss did not improve from 0.12312\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1257 - denormalized_mae: 159.6269 - val_loss: 0.1238 - val_denormalized_mae: 159.1482\n",
      "Epoch 140/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1602 - denormalized_mae: 169.1792\n",
      "Epoch 140: val_loss improved from 0.12312 to 0.12279, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1238 - denormalized_mae: 158.9499 - val_loss: 0.1228 - val_denormalized_mae: 159.1982\n",
      "Epoch 141/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1065 - denormalized_mae: 155.3663\n",
      "Epoch 141: val_loss did not improve from 0.12279\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1246 - denormalized_mae: 159.4815 - val_loss: 0.1228 - val_denormalized_mae: 159.3982\n",
      "Epoch 142/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1097 - denormalized_mae: 155.0631\n",
      "Epoch 142: val_loss did not improve from 0.12279\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1239 - denormalized_mae: 158.8642 - val_loss: 0.1228 - val_denormalized_mae: 159.3349\n",
      "Epoch 143/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1243 - denormalized_mae: 159.7252\n",
      "Epoch 143: val_loss did not improve from 0.12279\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1251 - denormalized_mae: 159.2106 - val_loss: 0.1231 - val_denormalized_mae: 158.8429\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1238 - denormalized_mae: 158.7673\n",
      "Epoch 144: val_loss improved from 0.12279 to 0.12270, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1238 - denormalized_mae: 158.7673 - val_loss: 0.1227 - val_denormalized_mae: 159.6116\n",
      "Epoch 145/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1373 - denormalized_mae: 161.5598\n",
      "Epoch 145: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1248 - denormalized_mae: 159.4639 - val_loss: 0.1234 - val_denormalized_mae: 159.5447\n",
      "Epoch 146/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1214 - denormalized_mae: 158.5842\n",
      "Epoch 146: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1239 - denormalized_mae: 159.2555 - val_loss: 0.1233 - val_denormalized_mae: 159.2768\n",
      "Epoch 147/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1231 - denormalized_mae: 158.8662\n",
      "Epoch 147: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1239 - denormalized_mae: 159.0757 - val_loss: 0.1235 - val_denormalized_mae: 158.7982\n",
      "Epoch 148/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1254 - denormalized_mae: 159.7086\n",
      "Epoch 148: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1240 - denormalized_mae: 158.6336 - val_loss: 0.1235 - val_denormalized_mae: 159.3423\n",
      "Epoch 149/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1367 - denormalized_mae: 162.0747\n",
      "Epoch 149: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1248 - denormalized_mae: 159.5076 - val_loss: 0.1242 - val_denormalized_mae: 159.4020\n",
      "Epoch 150/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1072 - denormalized_mae: 154.7997\n",
      "Epoch 150: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1234 - denormalized_mae: 158.7265 - val_loss: 0.1240 - val_denormalized_mae: 159.2948\n",
      "Epoch 151/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1264 - denormalized_mae: 158.4485\n",
      "Epoch 151: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1221 - denormalized_mae: 158.3517 - val_loss: 0.1233 - val_denormalized_mae: 158.8508\n",
      "Epoch 152/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1446 - denormalized_mae: 161.4618\n",
      "Epoch 152: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1242 - denormalized_mae: 158.6878 - val_loss: 0.1237 - val_denormalized_mae: 159.5512\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1225 - denormalized_mae: 158.7914\n",
      "Epoch 153: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1225 - denormalized_mae: 158.7914 - val_loss: 0.1231 - val_denormalized_mae: 159.3026\n",
      "Epoch 154/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1268 - denormalized_mae: 159.2269\n",
      "Epoch 154: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1242 - denormalized_mae: 158.8063 - val_loss: 0.1236 - val_denormalized_mae: 158.8541\n",
      "Epoch 155/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1711 - denormalized_mae: 171.4692\n",
      "Epoch 155: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1235 - denormalized_mae: 158.9807 - val_loss: 0.1233 - val_denormalized_mae: 159.4763\n",
      "Epoch 156/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1258 - denormalized_mae: 159.2699\n",
      "Epoch 156: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1234 - denormalized_mae: 158.5747 - val_loss: 0.1239 - val_denormalized_mae: 159.0056\n",
      "Epoch 157/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1546 - denormalized_mae: 167.4102\n",
      "Epoch 157: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1244 - denormalized_mae: 158.9692 - val_loss: 0.1238 - val_denormalized_mae: 159.4834\n",
      "Epoch 158/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1598 - denormalized_mae: 168.5762\n",
      "Epoch 158: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1216 - denormalized_mae: 158.1906 - val_loss: 0.1232 - val_denormalized_mae: 159.5963\n",
      "Epoch 159/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1552 - denormalized_mae: 166.9745\n",
      "Epoch 159: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1238 - denormalized_mae: 159.1170 - val_loss: 0.1231 - val_denormalized_mae: 158.8262\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1239 - denormalized_mae: 158.6325\n",
      "Epoch 160: val_loss did not improve from 0.12270\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1239 - denormalized_mae: 158.6325 - val_loss: 0.1236 - val_denormalized_mae: 159.5077\n",
      "Epoch 161/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1370 - denormalized_mae: 163.8761\n",
      "Epoch 161: val_loss improved from 0.12270 to 0.12257, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1238 - denormalized_mae: 158.9566 - val_loss: 0.1226 - val_denormalized_mae: 158.8560\n",
      "Epoch 162/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1273 - denormalized_mae: 160.4218\n",
      "Epoch 162: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1226 - denormalized_mae: 158.6355 - val_loss: 0.1227 - val_denormalized_mae: 158.7338\n",
      "Epoch 163/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1152 - denormalized_mae: 157.2363\n",
      "Epoch 163: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1242 - denormalized_mae: 158.5657 - val_loss: 0.1236 - val_denormalized_mae: 159.1130\n",
      "Epoch 164/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1210 - denormalized_mae: 158.9431\n",
      "Epoch 164: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1223 - denormalized_mae: 158.5203 - val_loss: 0.1238 - val_denormalized_mae: 159.5334\n",
      "Epoch 165/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1421 - denormalized_mae: 164.4667\n",
      "Epoch 165: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1237 - denormalized_mae: 158.7616 - val_loss: 0.1237 - val_denormalized_mae: 158.8636\n",
      "Epoch 166/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1834 - denormalized_mae: 175.0009\n",
      "Epoch 166: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1236 - denormalized_mae: 158.9531 - val_loss: 0.1230 - val_denormalized_mae: 159.1981\n",
      "Epoch 167/500\n",
      " 9/12 [=====================>........] - ETA: 0s - loss: 0.1223 - denormalized_mae: 158.6980\n",
      "Epoch 167: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1231 - denormalized_mae: 158.7863 - val_loss: 0.1234 - val_denormalized_mae: 159.0085\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1228 - denormalized_mae: 158.5671\n",
      "Epoch 168: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1228 - denormalized_mae: 158.5671 - val_loss: 0.1231 - val_denormalized_mae: 159.6638\n",
      "Epoch 169/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1409 - denormalized_mae: 163.5933\n",
      "Epoch 169: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1230 - denormalized_mae: 158.6602 - val_loss: 0.1235 - val_denormalized_mae: 158.9880\n",
      "Epoch 170/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1476 - denormalized_mae: 165.7684\n",
      "Epoch 170: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1222 - denormalized_mae: 158.1027 - val_loss: 0.1236 - val_denormalized_mae: 159.5268\n",
      "Epoch 171/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1205 - denormalized_mae: 158.2714\n",
      "Epoch 171: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1232 - denormalized_mae: 158.9774 - val_loss: 0.1230 - val_denormalized_mae: 159.0819\n",
      "Epoch 172/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1112 - denormalized_mae: 155.7452\n",
      "Epoch 172: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1223 - denormalized_mae: 158.1513 - val_loss: 0.1233 - val_denormalized_mae: 159.1490\n",
      "Epoch 173/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1071 - denormalized_mae: 155.3675\n",
      "Epoch 173: val_loss improved from 0.12257 to 0.12257, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1228 - denormalized_mae: 158.6037 - val_loss: 0.1226 - val_denormalized_mae: 159.1909\n",
      "Epoch 174/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1394 - denormalized_mae: 162.4772\n",
      "Epoch 174: val_loss did not improve from 0.12257\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1224 - denormalized_mae: 158.3560 - val_loss: 0.1230 - val_denormalized_mae: 158.9135\n",
      "Epoch 175/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1012 - denormalized_mae: 151.6050\n",
      "Epoch 175: val_loss improved from 0.12257 to 0.12244, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1210 - denormalized_mae: 158.1137 - val_loss: 0.1224 - val_denormalized_mae: 158.9601\n",
      "Epoch 176/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1163 - denormalized_mae: 157.3459\n",
      "Epoch 176: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1213 - denormalized_mae: 158.3684 - val_loss: 0.1228 - val_denormalized_mae: 158.5861\n",
      "Epoch 177/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1666 - denormalized_mae: 170.2417\n",
      "Epoch 177: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1233 - denormalized_mae: 158.4445 - val_loss: 0.1229 - val_denormalized_mae: 159.1255\n",
      "Epoch 178/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1065 - denormalized_mae: 153.1528\n",
      "Epoch 178: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1231 - denormalized_mae: 158.2936 - val_loss: 0.1232 - val_denormalized_mae: 158.7030\n",
      "Epoch 179/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0764 - denormalized_mae: 144.7172\n",
      "Epoch 179: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1219 - denormalized_mae: 158.3647 - val_loss: 0.1228 - val_denormalized_mae: 159.1149\n",
      "Epoch 180/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1324 - denormalized_mae: 162.2551\n",
      "Epoch 180: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1223 - denormalized_mae: 158.2250 - val_loss: 0.1233 - val_denormalized_mae: 158.9187\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1222 - denormalized_mae: 158.2215\n",
      "Epoch 181: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1222 - denormalized_mae: 158.2215 - val_loss: 0.1227 - val_denormalized_mae: 158.7079\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1219 - denormalized_mae: 158.1621\n",
      "Epoch 182: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1219 - denormalized_mae: 158.1621 - val_loss: 0.1224 - val_denormalized_mae: 158.7405\n",
      "Epoch 183/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1358 - denormalized_mae: 160.6627\n",
      "Epoch 183: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1213 - denormalized_mae: 158.0172 - val_loss: 0.1229 - val_denormalized_mae: 158.8799\n",
      "Epoch 184/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1483 - denormalized_mae: 162.2703\n",
      "Epoch 184: val_loss did not improve from 0.12244\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1213 - denormalized_mae: 158.3276 - val_loss: 0.1234 - val_denormalized_mae: 159.1569\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1222 - denormalized_mae: 158.2012\n",
      "Epoch 185: val_loss improved from 0.12244 to 0.12235, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1222 - denormalized_mae: 158.2012 - val_loss: 0.1224 - val_denormalized_mae: 158.7570\n",
      "Epoch 186/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1366 - denormalized_mae: 159.7458\n",
      "Epoch 186: val_loss did not improve from 0.12235\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1225 - denormalized_mae: 158.1717 - val_loss: 0.1226 - val_denormalized_mae: 158.9894\n",
      "Epoch 187/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1734 - denormalized_mae: 173.6799\n",
      "Epoch 187: val_loss improved from 0.12235 to 0.12196, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1224 - denormalized_mae: 158.6296 - val_loss: 0.1220 - val_denormalized_mae: 159.2578\n",
      "Epoch 188/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1448 - denormalized_mae: 165.4591\n",
      "Epoch 188: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1226 - denormalized_mae: 158.4440 - val_loss: 0.1228 - val_denormalized_mae: 158.0673\n",
      "Epoch 189/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1363 - denormalized_mae: 161.6484\n",
      "Epoch 189: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1229 - denormalized_mae: 158.2683 - val_loss: 0.1226 - val_denormalized_mae: 158.5618\n",
      "Epoch 190/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1174 - denormalized_mae: 157.4496\n",
      "Epoch 190: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1226 - denormalized_mae: 158.5656 - val_loss: 0.1236 - val_denormalized_mae: 159.0240\n",
      "Epoch 191/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1198 - denormalized_mae: 158.2332\n",
      "Epoch 191: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1218 - denormalized_mae: 158.4667 - val_loss: 0.1228 - val_denormalized_mae: 158.9025\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1189 - denormalized_mae: 157.5045\n",
      "Epoch 192: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1189 - denormalized_mae: 157.5045 - val_loss: 0.1225 - val_denormalized_mae: 159.3164\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1196 - denormalized_mae: 157.8495\n",
      "Epoch 193: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1196 - denormalized_mae: 157.8495 - val_loss: 0.1228 - val_denormalized_mae: 158.5844\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1215 - denormalized_mae: 157.8624\n",
      "Epoch 194: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1215 - denormalized_mae: 157.8624 - val_loss: 0.1230 - val_denormalized_mae: 158.8728\n",
      "Epoch 195/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1208 - denormalized_mae: 156.4505\n",
      "Epoch 195: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1229 - denormalized_mae: 158.3118 - val_loss: 0.1231 - val_denormalized_mae: 159.3374\n",
      "Epoch 196/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0891 - denormalized_mae: 149.6388\n",
      "Epoch 196: val_loss did not improve from 0.12196\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1222 - denormalized_mae: 159.0575 - val_loss: 0.1226 - val_denormalized_mae: 159.3445\n",
      "Epoch 197/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0980 - denormalized_mae: 151.6036\n",
      "Epoch 197: val_loss improved from 0.12196 to 0.12158, saving model to ./tmp\\NN_weights_best.h5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1208 - denormalized_mae: 157.9379 - val_loss: 0.1216 - val_denormalized_mae: 158.6328\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1200 - denormalized_mae: 157.8358\n",
      "Epoch 198: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1200 - denormalized_mae: 157.8358 - val_loss: 0.1225 - val_denormalized_mae: 158.6091\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1222 - denormalized_mae: 157.9337\n",
      "Epoch 199: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1222 - denormalized_mae: 157.9337 - val_loss: 0.1245 - val_denormalized_mae: 159.4922\n",
      "Epoch 200/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1186 - denormalized_mae: 157.7308\n",
      "Epoch 200: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1217 - denormalized_mae: 158.6116 - val_loss: 0.1241 - val_denormalized_mae: 159.6217\n",
      "Epoch 201/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1441 - denormalized_mae: 164.4183\n",
      "Epoch 201: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1213 - denormalized_mae: 157.9086 - val_loss: 0.1238 - val_denormalized_mae: 159.2862\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1210 - denormalized_mae: 158.3592\n",
      "Epoch 202: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1210 - denormalized_mae: 158.3592 - val_loss: 0.1236 - val_denormalized_mae: 159.0614\n",
      "Epoch 203/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1017 - denormalized_mae: 152.0996\n",
      "Epoch 203: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1211 - denormalized_mae: 157.7697 - val_loss: 0.1237 - val_denormalized_mae: 159.1029\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1202 - denormalized_mae: 158.0415\n",
      "Epoch 204: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1202 - denormalized_mae: 158.0415 - val_loss: 0.1232 - val_denormalized_mae: 159.3832\n",
      "Epoch 205/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1008 - denormalized_mae: 154.5690\n",
      "Epoch 205: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1202 - denormalized_mae: 157.5908 - val_loss: 0.1229 - val_denormalized_mae: 158.6866\n",
      "Epoch 206/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0859 - denormalized_mae: 148.6395\n",
      "Epoch 206: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1228 - denormalized_mae: 158.6335 - val_loss: 0.1228 - val_denormalized_mae: 159.8219\n",
      "Epoch 207/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1190 - denormalized_mae: 159.0376\n",
      "Epoch 207: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1208 - denormalized_mae: 158.2449 - val_loss: 0.1221 - val_denormalized_mae: 158.0486\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1205 - denormalized_mae: 157.7697\n",
      "Epoch 208: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1205 - denormalized_mae: 157.7697 - val_loss: 0.1222 - val_denormalized_mae: 158.3554\n",
      "Epoch 209/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1032 - denormalized_mae: 154.1070\n",
      "Epoch 209: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1217 - denormalized_mae: 158.2648 - val_loss: 0.1225 - val_denormalized_mae: 158.0925\n",
      "Epoch 210/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1099 - denormalized_mae: 153.7022\n",
      "Epoch 210: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1196 - denormalized_mae: 157.6695 - val_loss: 0.1233 - val_denormalized_mae: 158.7484\n",
      "Epoch 211/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1010 - denormalized_mae: 153.7046\n",
      "Epoch 211: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1209 - denormalized_mae: 157.8451 - val_loss: 0.1240 - val_denormalized_mae: 159.4584\n",
      "Epoch 212/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1312 - denormalized_mae: 160.9193\n",
      "Epoch 212: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1218 - denormalized_mae: 158.3674 - val_loss: 0.1239 - val_denormalized_mae: 159.4680\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1206 - denormalized_mae: 158.2957\n",
      "Epoch 213: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1206 - denormalized_mae: 158.2957 - val_loss: 0.1229 - val_denormalized_mae: 158.9205\n",
      "Epoch 214/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0838 - denormalized_mae: 147.2734\n",
      "Epoch 214: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1197 - denormalized_mae: 157.8038 - val_loss: 0.1221 - val_denormalized_mae: 157.8799\n",
      "Epoch 215/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1453 - denormalized_mae: 164.0019\n",
      "Epoch 215: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1202 - denormalized_mae: 157.4931 - val_loss: 0.1241 - val_denormalized_mae: 159.2507\n",
      "Epoch 216/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1252 - denormalized_mae: 158.9865\n",
      "Epoch 216: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1217 - denormalized_mae: 158.7261 - val_loss: 0.1237 - val_denormalized_mae: 159.7767\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1193 - denormalized_mae: 157.6086\n",
      "Epoch 217: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1193 - denormalized_mae: 157.6086 - val_loss: 0.1249 - val_denormalized_mae: 158.8443\n",
      "Epoch 218/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0923 - denormalized_mae: 150.6553\n",
      "Epoch 218: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1209 - denormalized_mae: 157.4899 - val_loss: 0.1248 - val_denormalized_mae: 159.7050\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1195 - denormalized_mae: 157.7463\n",
      "Epoch 219: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1195 - denormalized_mae: 157.7463 - val_loss: 0.1243 - val_denormalized_mae: 159.7045\n",
      "Epoch 220/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1106 - denormalized_mae: 155.2250\n",
      "Epoch 220: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1195 - denormalized_mae: 157.9448 - val_loss: 0.1239 - val_denormalized_mae: 158.8337\n",
      "Epoch 221/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1433 - denormalized_mae: 162.9188\n",
      "Epoch 221: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1208 - denormalized_mae: 157.6868 - val_loss: 0.1233 - val_denormalized_mae: 159.3281\n",
      "Epoch 222/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1615 - denormalized_mae: 169.2278\n",
      "Epoch 222: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1206 - denormalized_mae: 158.2998 - val_loss: 0.1236 - val_denormalized_mae: 159.5000\n",
      "Epoch 223/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1351 - denormalized_mae: 161.8548\n",
      "Epoch 223: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1192 - denormalized_mae: 157.5430 - val_loss: 0.1233 - val_denormalized_mae: 159.1920\n",
      "Epoch 224/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0891 - denormalized_mae: 150.2702\n",
      "Epoch 224: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1200 - denormalized_mae: 157.9518 - val_loss: 0.1230 - val_denormalized_mae: 159.1817\n",
      "Epoch 225/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1542 - denormalized_mae: 168.2961\n",
      "Epoch 225: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1197 - denormalized_mae: 157.7455 - val_loss: 0.1230 - val_denormalized_mae: 158.8835\n",
      "Epoch 226/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1480 - denormalized_mae: 166.0537\n",
      "Epoch 226: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1192 - denormalized_mae: 157.1789 - val_loss: 0.1233 - val_denormalized_mae: 158.6940\n",
      "Epoch 227/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1232 - denormalized_mae: 159.9062\n",
      "Epoch 227: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.1193 - denormalized_mae: 157.4362 - val_loss: 0.1241 - val_denormalized_mae: 159.5743\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1192 - denormalized_mae: 157.7830\n",
      "Epoch 228: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1192 - denormalized_mae: 157.7830 - val_loss: 0.1239 - val_denormalized_mae: 159.0882\n",
      "Epoch 229/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1187 - denormalized_mae: 157.5793\n",
      "Epoch 229: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1192 - denormalized_mae: 157.6054 - val_loss: 0.1235 - val_denormalized_mae: 159.6344\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1180 - denormalized_mae: 157.3536\n",
      "Epoch 230: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1180 - denormalized_mae: 157.3536 - val_loss: 0.1236 - val_denormalized_mae: 158.8489\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1211 - denormalized_mae: 157.5562\n",
      "Epoch 231: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1211 - denormalized_mae: 157.5562 - val_loss: 0.1236 - val_denormalized_mae: 159.1165\n",
      "Epoch 232/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1149 - denormalized_mae: 156.7784\n",
      "Epoch 232: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1198 - denormalized_mae: 158.0884 - val_loss: 0.1233 - val_denormalized_mae: 159.3610\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1182 - denormalized_mae: 157.3533\n",
      "Epoch 233: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1182 - denormalized_mae: 157.3533 - val_loss: 0.1237 - val_denormalized_mae: 158.5856\n",
      "Epoch 234/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1169 - denormalized_mae: 156.7152\n",
      "Epoch 234: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1170 - denormalized_mae: 156.7335 - val_loss: 0.1224 - val_denormalized_mae: 159.0713\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1213 - denormalized_mae: 157.9160\n",
      "Epoch 235: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1213 - denormalized_mae: 157.9160 - val_loss: 0.1234 - val_denormalized_mae: 158.8988\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1200 - denormalized_mae: 158.0217\n",
      "Epoch 236: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1200 - denormalized_mae: 158.0217 - val_loss: 0.1235 - val_denormalized_mae: 159.4143\n",
      "Epoch 237/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1339 - denormalized_mae: 161.1908\n",
      "Epoch 237: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1187 - denormalized_mae: 157.5745 - val_loss: 0.1224 - val_denormalized_mae: 159.0826\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1192 - denormalized_mae: 157.2773\n",
      "Epoch 238: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1192 - denormalized_mae: 157.2773 - val_loss: 0.1238 - val_denormalized_mae: 158.8330\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1194 - denormalized_mae: 157.4555\n",
      "Epoch 239: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1194 - denormalized_mae: 157.4555 - val_loss: 0.1236 - val_denormalized_mae: 160.1337\n",
      "Epoch 240/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1199 - denormalized_mae: 157.9348\n",
      "Epoch 240: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1195 - denormalized_mae: 157.8394 - val_loss: 0.1238 - val_denormalized_mae: 159.0334\n",
      "Epoch 241/500\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1112 - denormalized_mae: 152.8093\n",
      "Epoch 241: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1178 - denormalized_mae: 156.8370 - val_loss: 0.1237 - val_denormalized_mae: 159.0898\n",
      "Epoch 242/500\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 0.1143 - denormalized_mae: 155.9618\n",
      "Epoch 242: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1196 - denormalized_mae: 157.5818 - val_loss: 0.1246 - val_denormalized_mae: 159.4553\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1191 - denormalized_mae: 157.2822\n",
      "Epoch 243: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1191 - denormalized_mae: 157.2822 - val_loss: 0.1247 - val_denormalized_mae: 159.4373\n",
      "Epoch 244/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1222 - denormalized_mae: 158.7133\n",
      "Epoch 244: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1189 - denormalized_mae: 157.7110 - val_loss: 0.1236 - val_denormalized_mae: 159.0718\n",
      "Epoch 245/500\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1140 - denormalized_mae: 156.1447\n",
      "Epoch 245: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1183 - denormalized_mae: 157.2507 - val_loss: 0.1237 - val_denormalized_mae: 158.7355\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1183 - denormalized_mae: 157.3078\n",
      "Epoch 246: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1183 - denormalized_mae: 157.3078 - val_loss: 0.1230 - val_denormalized_mae: 159.0476\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1195 - denormalized_mae: 157.6954\n",
      "Epoch 247: val_loss did not improve from 0.12158\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1195 - denormalized_mae: 157.6954 - val_loss: 0.1235 - val_denormalized_mae: 158.5525\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABx2klEQVR4nO3dd3zV1f3H8de9N3sTEjIgEDYywiYiKo4ouAcqbkSrdVC1tNbSX+tqK1appa1U1LqqVpy4xSIIiLIh7L0SErLJ3rnf3x8nuUkkIQmE3BDez8fjPnJz7/fee+6XkPvOOZ9zjs2yLAsRERGRdszu7gaIiIiINEWBRURERNo9BRYRERFp9xRYREREpN1TYBEREZF2T4FFRERE2j0FFhEREWn3FFhERESk3fNwdwNai9PpJDU1lcDAQGw2m7ubIyIiIs1gWRYFBQVER0djtzfej9JhAktqaioxMTHuboaIiIgch+TkZLp169bo/R0msAQGBgLmDQcFBbm5NSIiItIc+fn5xMTEuD7HG9NhAkvNMFBQUJACi4iIyCmmqXIOFd2KiIhIu6fAIiIiIu2eAouIiIi0ewosIiIi0u4psIiIiEi7p8AiIiIi7Z4Ci4iIiLR7CiwiIiLS7imwiIiISLunwCIiIiLtngKLiIiItHsKLCIiItLudZjND0+W5/+3k7ySCu4/vw8RQT7ubo6IiMhpST0sTZi3Jpk3Vxwku7Dc3U0RERE5bSmwNMHDbra7rnJabm6JiIjI6UuBpQkOhwksFU6nm1siIiJy+lJgaYKH3Zwi9bCIiIi4jwJLE2qGhCqrFFhERETcRYGlCQ7VsIiIiLidAksTPKprWCpVwyIiIuI2CixNcFTXsGhISERExH0UWJrgqmHRkJCIiIjbKLA0QeuwiIiIuJ8CSxNUwyIiIuJ+CixNcGgdFhEREbdTYGmC1mERERFxPwWWJjhUdCsiIuJ2CixN8HTUFN2qhkVERMRdFFia4FqHRT0sIiIibqPA0gRNaxYREXE/BZYm1NSwVKjoVkRExG0UWJpQ28OiGhYRERF3UWBpQu3CcephERERcRcFliZ4aOE4ERERt1NgaYLWYREREXE/BZYm1K50qxoWERERd1FgaYJ6WERERNzvuALLnDlziI2NxcfHh/j4eFavXt3osVu3bmXSpEnExsZis9mYPXv2UcfU3PfTywMPPHA8zWtVHg7VsIiIiLhbiwPLe++9x/Tp03n88cdZv349Q4cOZcKECWRkZDR4fHFxMb169eKZZ54hMjKywWPWrFnD4cOHXZeFCxcCcP3117e0ea3OQz0sIiIibtfiwPL8889z9913M3XqVAYOHMjcuXPx8/Pjtddea/D40aNH89xzz3HjjTfi7e3d4DHh4eFERka6Ll988QW9e/dm/PjxLW1eq6sZEqrSwnEiIiJu06LAUl5ezrp160hISKh9ArudhIQEVqxY0SoNKi8v5+233+bOO+/EZrM1elxZWRn5+fn1LidDTQ9LhRaOExERcZsWBZasrCyqqqqIiIiod3tERARpaWmt0qBPPvmE3Nxc7rjjjmMeN3PmTIKDg12XmJiYVnn9n3JoLyERERG3a3ezhF599VUuueQSoqOjj3ncjBkzyMvLc12Sk5NPSns8HdqtWURExN08WnJwWFgYDoeD9PT0erenp6c3WlDbEgcPHuTbb7/l448/bvJYb2/vRmtiWpNqWERERNyvRT0sXl5ejBw5kkWLFrluczqdLFq0iLFjx55wY15//XW6dOnCZZdddsLP1Vo0S0hERMT9WtTDAjB9+nSmTJnCqFGjGDNmDLNnz6aoqIipU6cCcPvtt9O1a1dmzpwJmCLabdu2ua6npKSQmJhIQEAAffr0cT2v0+nk9ddfZ8qUKXh4tLhZJ03twnEquhUREXGXFieDyZMnk5mZyWOPPUZaWhrDhg1jwYIFrkLcpKQk7PbajpvU1FSGDx/u+n7WrFnMmjWL8ePHs2TJEtft3377LUlJSdx5550n8HZaX81uzSq6FRERcR+bZVkd4pM4Pz+f4OBg8vLyCAoKarXn/XxjKr94dwNje3Xm3XvObLXnFRERkeZ/fre7WULtjYemNYuIiLidAksTVMMiIiLifgosTaipYdEsIREREfdRYGmCo7qAuFLrsIiIiLiNAksTPFXDIiIi4nYKLE1QDYuIiIj7KbA0QeuwiIiIuJ8CSxNqalgqVMMiIiLiNgosTdA6LCIiIu6nwNIETWsWERFxPwWWJtT2sKjoVkRExF0UWJrgWodFPSwiIiJuo8DShJoeFi0cJyIi4j4KLE1wqOhWRETE7RRYmlBbdKsaFhEREXdRYGmCR3UNi9MCp3pZRERE3EKBpQk1Q0IAVZYCi4iIiDsosDTBo05gUeGtiIiIeyiwNKFuD4vqWERERNxDgaUJno7aU6SZQiIiIu6hwNKEOh0sWjxORETETRRYmmCz2bQBooiIiJspsDRDTR1LRZVqWERERNxBgaUZ1MMiIiLiXgoszeDh0AaIIiIi7qTA0gzqYREREXEvBZZmcGjHZhEREbdSYGmGmh4WLRwnIiLiHgoszeBw7disHhYRERF3UGBpBs/qHZtVwyIiIuIeCizNoBoWERER91JgaQaHZgmJiIi4lQJLM3hU17BUqOhWRETELRRYmsFRU8OiISERERG3UGBpBk+7ZgmJiIi4kwJLM6iGRURExL0UWJrBw6GF40RERNxJgaUZampYNK1ZRETEPRRYmkGbH4qIiLiXAkszeKjoVkRExK0UWJqhpoalSjUsIiIibqHA0gyuGhb1sIiIiLiFAkszeGgvIREREbdSYGkGh2pYRERE3EqBpRk8VcMiIiLiVgoszaAeFhEREfdSYGkGj5rNDxVYRERE3OK4AsucOXOIjY3Fx8eH+Ph4Vq9e3eixW7duZdKkScTGxmKz2Zg9e3aDx6WkpHDrrbfSuXNnfH19GTJkCGvXrj2e5rW6mh6WChXdioiIuEWLA8t7773H9OnTefzxx1m/fj1Dhw5lwoQJZGRkNHh8cXExvXr14plnniEyMrLBY44cOcK4cePw9PTk66+/Ztu2bfz1r3+lU6dOLW3eSVG70q1qWERERNzBo6UPeP7557n77ruZOnUqAHPnzuXLL7/ktdde47e//e1Rx48ePZrRo0cDNHg/wF/+8hdiYmJ4/fXXXbf17NmzpU07aWo3P1QPi4iIiDu0qIelvLycdevWkZCQUPsEdjsJCQmsWLHiuBvx2WefMWrUKK6//nq6dOnC8OHDeeWVV475mLKyMvLz8+tdThaHalhERETcqkWBJSsri6qqKiIiIurdHhERQVpa2nE3Yt++fbz44ov07duXb775hvvuu48HH3yQN998s9HHzJw5k+DgYNclJibmuF+/KdpLSERExL3axSwhp9PJiBEjePrppxk+fDj33HMPd999N3Pnzm30MTNmzCAvL891SU5OPmntc01rrlINi4iIiDu0KLCEhYXhcDhIT0+vd3t6enqjBbXNERUVxcCBA+vddsYZZ5CUlNToY7y9vQkKCqp3OVnUwyIiIuJeLQosXl5ejBw5kkWLFrluczqdLFq0iLFjxx53I8aNG8fOnTvr3bZr1y569Ohx3M/ZmjwcqmERERFxpxbPEpo+fTpTpkxh1KhRjBkzhtmzZ1NUVOSaNXT77bfTtWtXZs6cCZhC3W3btrmup6SkkJiYSEBAAH369AHgl7/8JWeddRZPP/00N9xwA6tXr+bll1/m5Zdfbq33eULUwyIiIuJeLQ4skydPJjMzk8cee4y0tDSGDRvGggULXIW4SUlJ2O21HTepqakMHz7c9f2sWbOYNWsW48ePZ8mSJYCZ+jx//nxmzJjBU089Rc+ePZk9eza33HLLCb691lFTw1KlheNERETcwmZZVof4FM7Pzyc4OJi8vLxWr2eZtzqJ3368mYQzuvDvKaNb9blFREROZ839/G4Xs4TaO21+KCIi4l4KLM3gqaJbERERt1JgaYbadVgUWERERNxBgaUZajc/VGARERFxBwWWZqjpYanQbs0iIiJuocDSDDW7NauHRURExD0UWJrBo3pdGdWwiIiIuIcCSzOohkVERMS9FFiaoXYdFtWwiIiIuIMCSzPU1LBo4TgRERH3UGBpBodqWERERNxKgaUZVMMiIiLiXgoszaAhIREREfdSYGmG2h4WFd2KiIi4gwJLM6iGRURExL0UWJrBw64hIREREXdSYGkGLc0vIiLiXgoszaCF40RERNxLgaUZavYSclrgVC+LiIhIm1NgaQY/L4fren5phRtbIiIicnpSYGkGH08Hnf29AEjNLXVza0RERE4/CizNFBnsA0BafombWyIiInL6UWBppqhgX0A9LCIiIu6gwNJM0SGmh+VwnnpYRERE2poCSzPVDAkdzlMPi4iISFtTYGmm6OohocMaEhIREWlzCizNVNvDoiEhERGRtqbA0kyuHpa8UixLi8eJiIi0JQWWZooI9gagrNLJkWItHiciItKWFFiaydvDQVhAzeJxGhYSERFpSwosLRBVZ1hIRERE2o4CSwtE1ax2q8JbERGRNqXA0gI1gSVVPSwiIiJtSoGlBaJCatZiUQ+LiIhIW1JgaYEorXYrIiLiFgosLRBd3cOSoh4WERGRNqXA0gIxnfwA08NSUeV0c2tEREROHwosLdAl0BsvDztVTkt7ComIiLQhBZYWsNttxHQyw0JJOcVubo2IiMjpQ4GlhbqHmmEhBRYREZG2o8DSQgosIiIibU+BpYViqgNLsgKLiIhIm1FgaaEenf0B9bCIiIi0JQWWFtKQkIiISNtTYGmhmFAzSyivpIK84go3t0ZEROT0oMDSQn5eHoQFeAOQfES9LCIiIm1BgeU4dK/uZTmYrcAiIiLSFo4rsMyZM4fY2Fh8fHyIj49n9erVjR67detWJk2aRGxsLDabjdmzZx91zBNPPIHNZqt3GTBgwPE0rU2ojkVERKRttTiwvPfee0yfPp3HH3+c9evXM3ToUCZMmEBGRkaDxxcXF9OrVy+eeeYZIiMjG33eQYMGcfjwYddl+fLlLW1am1FgERERaVstDizPP/88d999N1OnTmXgwIHMnTsXPz8/XnvttQaPHz16NM899xw33ngj3t7ejT6vh4cHkZGRrktYWFhLm9ZmulUHlkOqYREREWkTLQos5eXlrFu3joSEhNonsNtJSEhgxYoVJ9SQ3bt3Ex0dTa9evbjllltISko6oec7maKCfQBIz9cGiCIiIm2hRYElKyuLqqoqIiIi6t0eERFBWlracTciPj6eN954gwULFvDiiy+yf/9+zjnnHAoKChp9TFlZGfn5+fUubSUyyASWw3kKLCIiIm3Bw90NALjkkktc1+Pi4oiPj6dHjx68//773HXXXQ0+ZubMmTz55JNt1cR6Iqt7WApKKykqq8Tfu12cRhERkQ6rRT0sYWFhOBwO0tPT692enp5+zILalgoJCaFfv37s2bOn0WNmzJhBXl6e65KcnNxqr9+UQB9PAqpDSpqGhURERE66FgUWLy8vRo4cyaJFi1y3OZ1OFi1axNixY1utUYWFhezdu5eoqKhGj/H29iYoKKjepS1FBJkC4jQNC4mIiJx0LZ4lNH36dF555RXefPNNtm/fzn333UdRURFTp04F4Pbbb2fGjBmu48vLy0lMTCQxMZHy8nJSUlJITEys13vy61//mqVLl3LgwAF+/PFHrrnmGhwOBzfddFMrvMWTIyrYLB6nwCIiInLytbj4YvLkyWRmZvLYY4+RlpbGsGHDWLBggasQNykpCbu9NgelpqYyfPhw1/ezZs1i1qxZjB8/niVLlgBw6NAhbrrpJrKzswkPD+fss89m5cqVhIeHn+DbO3lq6lg0JCQiInLy2SzLstzdiNaQn59PcHAweXl5bTI8NOubnbzw3R5uPbM7f7p6yEl/PRERkY6ouZ/f2kvoOLl6WPLK3NwSERGRjk+B5ThFuYaEStzcEhERkY5PgeU4RQTV9LCohkVERORkU2A5TjU9LFmF5ZRXOt3cGhERkY5NgeU4hfp74eUwp097ComIiJxcCizHyWazERFcvXicAouIiMhJpcByAqKCtHiciIhIW1BgOQE1U5sP52mmkIiIyMmkwHICenT2A2BnWqGbWyIiItKxKbCcgBE9OgGw5kCOm1siIiLSsSmwnICRPTphs0FSTrFmComIiJxECiwnIMjHkzMizb4H6mURERE5eRRYTtCYnqEArNmvwCIiInKyKLCcoNGxJrCsPnDEzS0RERHpuBRYTtDonqbwdkdaPnklFW5ujYiISMekwHKCugT6ENvZD8uCVfuy3d0cERGRDkmBpRVcMCACgPfWJLu5JSIiIh2TAksruPXM7gAs3plBck6xm1sjIiLS8SiwNGXLR7D5Q7CsRg/pFR7AOX3DsCx4e9XBNmyciIjI6UGB5VhKcuGr38BHd8FrEyFtS6OH3nZmDwDeX5NMaUVVGzVQRETk9KDAciwe3hD/c/D0g+SV8NY1UFne4KEXnhFBl0BvjhRXsD5JU5xFRERakwLLsXj6wvjfwLS14B0MRRmQuaPBQx12GyOr9xbakpLXlq0UERHp8BRYmiO4K0TFmetpmxs9bEi3YAA2HVJgERERaU0KLM0VOcR8TdvU6CFxXUMA2KweFhERkValwNJckc3oYelqelgOZheTV6xVb0VERFqLAktzuXpYNoPT2eAhwX6e9OjsB6iXRUREpDUpsDRXeH9weEFZPuQ2vtZKTS/LppTcNmqYiIhIx6fA0lwOT+hyhrl+rDqW6sLbzSq8FRERaTUKLC3RrDqWEEAzhURERFqTAktL1ASWw433sAzuGoSnw0ZKbonWYxEREWklCiwtETHQfM3a2eghgT6eXDwoEoD/rk5qi1aJiIh0eAosLREYZb4WZR3zsFvize7Nn25IobCs8mS3SkREpMNTYGkJv87ma3khVJQ0etjYXp3pFe5PUXkVn2xIaaPGiYiIdFwKLC3hEwx2T3P9GL0sNpuNW+LN7s3z1mhYSERE5EQpsLSEzQb+4eZ6UeYxD71meFfsNtiSkk9yTnEbNE5ERKTjUmBpKf8w87WJOpZQfy/G9AwF4JutaSe7VSIiIh2aAktL1QSW4mMHFoCJ1bOFFFhEREROjAJLSzVzSAhwTW9ee/AImQVlJ7NVIiIiHZoCS0u1ILBEh/gytFswlgX3vr2OO15fzZ6MwpPcQBERkY5HgaWlaqY2N1HDUmPiYLN2y7qDR1iyM5M3fzxwkhomIiLScXm4uwGnHFcPS/MCyx1nxVJSUcWBrCI+25jK1lQt1y8iItJS6mFpqRYMCQH4ejmYflE/HrywLwDbDxdQ5bROVutEREQ6JAWWlmphD0uNnmH++Ho6KKmoYn+W6lhERERaQoGlpfyra1iKs8Bqfk+Jw25jYHQQYBaTExERkeZTYGmpmh6WylKzp1ALDKoOLKpjERERaRkFlpby8gdPP3O9mXUsNQZHBwPqYREREWkpBZbj0czl+X9qUNfaHharBcNJIiIipzsFluPhd3yBpW+XQDwdNvJLKzl0pASAI0XlFJRWtHYLRUREOpTjCixz5swhNjYWHx8f4uPjWb16daPHbt26lUmTJhEbG4vNZmP27NnHfO5nnnkGm83Gww8/fDxNaxstnNpcw8vDTv/IQACW78kio6CUhOeXctk/lmuqs4iIyDG0OLC89957TJ8+nccff5z169czdOhQJkyYQEZGRoPHFxcX06tXL5555hkiIyOP+dxr1qzhpZdeIi4urqXNalvHGVgArhwaDcALi/fw3IKdZBeVk5RTTGpuSWu2UEREpENpcWB5/vnnufvuu5k6dSoDBw5k7ty5+Pn58dprrzV4/OjRo3nuuee48cYb8fb2bvR5CwsLueWWW3jllVfo1KlTS5vVtlxTm7Nb/NDbx8YSEeRNSm4JH6w75Lp9b6bWZhEREWlMiwJLeXk569atIyEhofYJ7HYSEhJYsWLFCTXkgQce4LLLLqv33MdSVlZGfn5+vUubqelhKWy4V+lYfDwdPHRhv6Nu35tZdKKtEhER6bBaFFiysrKoqqoiIiKi3u0RERGkpaUddyPmzZvH+vXrmTlzZrMfM3PmTIKDg12XmJiY4379Fgs0GxpScPi4Hn79qG4MiAzE19PBhEHmXO5TD4uIiEij3D5LKDk5mYceeoh33nkHHx+fZj9uxowZ5OXluS7JycknsZU/EdLdfM1NOq6HezrsfHjfWSz7zflcPNDU9WhISEREpHEt2q05LCwMh8NBenp6vdvT09ObLKhtzLp168jIyGDEiBGu26qqqli2bBkvvPACZWVlOByOox7n7e19zJqYkyq4ujcnPxWqKsHR8k2vA7w9CPD2oHeXAAD2aUhIRESkUS3qYfHy8mLkyJEsWrTIdZvT6WTRokWMHTv2uBpw4YUXsnnzZhITE12XUaNGccstt5CYmNhgWHG7gAhweIFVBfkpJ/RUvcL9AcgoKNN6LCIiIo1ocdfA9OnTmTJlCqNGjWLMmDHMnj2boqIipk6dCsDtt99O165dXfUo5eXlbNu2zXU9JSWFxMREAgIC6NOnD4GBgQwePLjea/j7+9O5c+ejbm837HbTy5Kz1wwLdepx3E8V5ONJeKA3mQVl7MssYmhMSOu1U0REpINocWCZPHkymZmZPPbYY6SlpTFs2DAWLFjgKsRNSkrCbq/tuElNTWX48OGu72fNmsWsWbMYP348S5YsOfF34C4h3WsDywnqFeZPZkEZ325PZ9b/dnJLfA8mDj6+ITYREZGOyGZ1kE1t8vPzCQ4OJi8vj6CgoJP/gp89COvfhPNmwHm/PaGnmvHxZt5dXRt8/LwcLHjoXLp39jvRVoqIiLRrzf38bnm1qBgh1YW3rdDD0ru6jqVGcXkVD87bQL+IAA5kFfPHqwe7lvQXERE5Hbl9WvMpK6S6bqU1Akv1TCGAmdcOIcDbg8TkXN5fe4jVB3K46ZWVbEnJI7+0Aqf2HBIRkdOQeliOl2stloMn/FTjeodx/chujIrtxOTR3Qny8eTpr7YztndndqTlsyUln8v/uRyAQdFBfPLAODwdypoiInL6UGA5XjWBJS/luNdiqeHlYee564e6vr8sLorL4sxqunnFFdzz1lpW7c8BYGtqPt9sTePyuOjjb7uIiMgpRn+mH6+ASLB7mrVYjnOJ/uYI9vPkvZ+PZeuTE3jwgj4AvLp8/0l7PRERkfZIgeV42e0Q3M1cb4U6lqb4e3tw29hYvBx2NiTlsj7pCADzNxyi54wvWbQ9vYlnEBEROXUpsJyIE9xTqKXCA725apgZCnqtupdl3upkLAs+SUxtkzaIiIi4gwLLiagJLDn72uwlp5wVC8DCbenkFpezITkXgA3VPS4iIiIdkQLLiYiqLpRNXtlmLzkoOoiuIb6UVTp5YfEeyiudABw6UkJGQWmbtUNERKQtKbCciJ7jzdekVVBR0iYvabPZuPCMLgD8Z0X9KdWJSblt0gYREZG2psByIsL6mtlCVWWQvLrNXvbCM8y+TeVVpncl0NtMqa4ZHhIREeloFFhOhM0Gvap7WfYva7OXPbNXKP5eDtf3t441q+6qjkVERDoqBZYT1fNc87UNA4u3h4Nz+oYDEBnkwzXDuwKw6VAeldW9LiIiIh2JAsuJqgksKeugrKDNXvaaESakTBwcSZ/wAAK9PSgur+LaF39k+nuJHCkqb7O2iIiInGwKLCcqpDt0ijUr3h5Y3mYvO2FQJIt+NZ4Zlw7AbrdxZu/OgOll+XhDCpNfXkF6vmYNiYhIx6DA0hr6Xmy+bv2kTV+2d3gA3h6mluUvk+L4503D+fuNw4gI8mZXeiE3vbLSNe25IQ/N28DFf1tKYVllWzVZRETkuCiwtIbBk8zXHV+02fTmnwr19+KKodFcNawrH957FmEB3uzLLOLDdYcA2JNRWC+87M8q4tPEVHalF7LuoIp1RUSkfVNgaQ3dxkBwDJQXwu7/ubs1xIT6cf95vQGY890envp8GwnPL+Xpr7a7jvmszlL+2w/nt3kbRUREWkKBpTXY7TD4WnN984fubUu1m+O7Ex7oTUpuCa/9YPYd+m5nBgCWZfHZxhTXsQosIiLS3imwtJaaYaFd30BRtnvbAvh4OrhvfO96tx3MLiarsIxth/PZm1nkun3H4bab3SQiInI8FFhaS2QcRAwxq95+Ng0sy90t4pYzu3P3OT35503D6dslAIANSbl8ttEMBw2NCQFgb2YhZZVV7mqmiIhIkxRYWovNBlfPAYcX7PwKVr/s7hbh7eHg/y4byBVDoxnRvRMAaw7k8OkGE1h+fm4vgn09qXRa7MkodGdTRUREjkmBpTVFDYWL/2SuL3ysXQwN1RjRIwSA/65KIi2/lE5+nlx4RhcGRAYCsF3DQiIi0o4psLS2MfeY4FJZChvecndrXGp6WGrWXLl2RDe8PRycERUEHF14m5FfylebD1NaoaEiERFxPwWW1mazmdACsOZVcLaPD/ze4QEE+Xi4vp88OgaAM6JMD8uONBNYSiuqeOzTLZz97Hfc/856pr6+RqFFRETcToHlZBg8CXw7QV5Su1iXBcButzG8updlePcQ+kWYoFLTw7ItNZ8qp8ULi/fwnxUHKa904rDbWLEvm7veXMMnG1JIzil2W/tFROT0psByMnj6wvBbzfVVc93bljpuGBVDkI8HD17Y13Vbv4hAgnw8OFJcwX9WHOA/Kw4A8OykOObdcya+ng5+2JPNw+8lcuHzS0lMznVP40VE5LRms6x2MP+2FeTn5xMcHExeXh5BQUHubg4cOQD/GGE2RbxrIcSMcXeLGvXysr08/dUObDYzG7tXuD/f/nI8druNjcm5vL3yIInJuezOKKRvlwA+/8XZ+Hg63N1sERHpAJr7+a0elpOlUywMu9lc/+5ptzalKbePjaVriK9r6Zh7z+2N3W4DzFotz10/lPd/PpawAC92ZxTy90W73dhaERE5HSmwnEznPgJ2D9j3HRz80d2taZSPp4NfT+gHQGSQD1cNjz7qmE7+Xvzp6iEAvLhkL59sSDnqGBERkZPFo+lD5Lh16mFqWda9AR9MhZvnQfRwd7eqQVcP64qXw0G/iAC8PRoe7pk4OJKp42J5/YcD/PqDjezLLKRbqB8TBkYS7OfZxi0WEZHTiWpYTrbCTPjPlZCxDTz94JYPIXacu1t13JxOi+nvJ/JJnd2ez+0Xzn/uHMOBrCJW789h0shuOKqHlERERI5FNSztRUA43LkAel8AFcUw/+dQduquKmu323ju+qH8/rIzmDSiG54OG8t2ZfLJhhQmv7yC33y0yTXTSEREpLUosLQFn2C44S0I6QF5yfC/P7i7RSfE02HnZ+f04q83DOX2sbEAPPxeIun5ZQC8tHTfMTdT7CCdeiIi0oYUWNqKdwBcNcdcX/c6HPjBve1pJb+4oI9rBV0fTzthAV6k5Zfy4bpDDR7/3pokBvxhAd/tzGjLZoqIyClOgaUt9TwHRt5hrn/7OHSAnoYQPy9+f9lA/LwcPHNtHA+c3weAf323l+LyynrHFpZVMvPrHZRVOvl4vWYZiYhI8ymwtLXzfmeKbw+tgR1furs1reKG0TFse2oiVw/vyk1juhMe6E1KbglTX19TL7S8+eMBcosrAFi9P1tDQyIi0mwKLG0tMALOvN9cX/QklOS6tTmtzcfTwUu3jSTA24NV+3NcoaWgtIJXvt/nOi49v4zknBLX92sO5PDj3iyFGBERaZACizuMexB8QyFrF7x4Fuxf5u4WtaoR3Tvxn7vGEFgdWqa8tpobXlpJbnEFvcL8GRoTAsDqAzkA7MkoYPJLK7j5lVXc/tpqkrK1yaKIiNSnwOIOPsFw64cQ2gvyU+Dt6yBrj7tb1apGdO/Em9WhZc2BI2w/nE9nfy+euz6OM3uFArBmvwksc5fuw1ndsfL97iwenLehWa+xL7OQvJKKk9J+ERFpXxRY3KXrSLh3OfQ8F6rK4POHwOl0d6taVU1PS3SwD+f1D+frh85hZI9QxsRWB5YDOaTmlvBpoinAfe66OAA2Hsolr/jYQeS7HRlc+PxSHnhn/cl9EyIi0i5oaX538vKHK1+Af50JB5fDvJvB0wfG3AM9znJ361rF8O6d+OG3F2Cz1a58O7JHJwD2ZRXx2483U1FlEd8zlOtHxfCvJXvZn1XE+qQjVFQ5eeKzrYzo0YlJI7sxvm84druNgtIKZny8GcuCVfuzKausanQ7ARER6RgUWNytUw+44A/wzQzY9bW57cAP8MAq8At1b9taSd2wAmYq9IDIQHakFbBsVyYA957XGzBhZn9WEWsP5rB8TzapeaWkbjrMF5sOMyg6iMmjY/h+dxZp+aUAVFRZ7EorZEi34LZ9UyIi0qYUWNqD+J+D3WFmDG3+ALJ3w4IZcO1LZq0WW8fbl+cPlw/knVUHCfD2YEjXYM7rFw7AqB6d+HDdIb7cdJgD2cXYbXBzfHfmr09ha2o+j3261fUc0cE+pOaVsiklV4FFRKSD0+aH7c2htfDqRWA5wcMH7J4w5DoYNRUi4zpkeKlrT0YhCc8vdX1/Zq9Q5t0zlpyicl5dvo/thwsI9PFg4qBINqfk8a8le7lpTAwzr41zY6tFROR4NffzWz0s7U23UXDWg/DDbKgsBUrNUv7rXofAKBh+G5w3A+wds166d7g/nfw8OVJddHtZXDQAof5ePDJhQL1ja7LbpkN5bdpGERFpex3zU+9Ul/AETFsLD22CKV/AGVea3paCw7DsWVj4hw6xrH9DbDabqyjXboOJgyIbPXZwVzMMtCu9gNKKxjdbFBGRU99xBZY5c+YQGxuLj48P8fHxrF69utFjt27dyqRJk4iNjcVmszF79uyjjnnxxReJi4sjKCiIoKAgxo4dy9dff308TesYbDYI62sKcnueA5PfgkcPwqWzzP0rXoC558Dfh8Ez3eHPUbDpA7c2uTWN6WmKjeN7diY80LvR47qG+BLq70VFlcXvP9nCmD9/y/MLdx1zp2gRETk1tTiwvPfee0yfPp3HH3+c9evXM3ToUCZMmEBGRsO77xYXF9OrVy+eeeYZIiMb/mu5W7duPPPMM6xbt461a9dywQUXcNVVV7F169YGjz8tefrAmLvh4j+b79M3w5H9UJoHFcXw6QOQss69bWwlt4+NZfpF/Zh57ZBjHmez2Vy9LB+uO0RGQRn/WLSbK//5AzlF5Y0+Lq+4gnmrk9QrIyJyCmlx0W18fDyjR4/mhRdeAMDpdBITE8MvfvELfvvb3x7zsbGxsTz88MM8/PDDTb5OaGgozz33HHfddVez2tVhim6bI3UD5B820559Q83Ozzu/gsBouHOB6Zk5Tcz6ZicvfGdWCb5+ZDcW78ggu6icaef34dcT+h91vGVZ3PH6GpbuyuTn5/ZixqVntOj1Xlu+n90ZhTx55SC8PDSiKiJyopr7+d2i37jl5eWsW7eOhISE2iew20lISGDFihXH39o6qqqqmDdvHkVFRYwdO7bR48rKysjPz693OW1ED4cBl0L3MyG8H1zzEoT1h4JUeOUCSFrl7ha2mauHd2VQdBBPXDGQ564fyp+uHgzAu6uTGhwa+m5nBkur1375cN0hKqqav7pwZZWTZ77ewburk1i4Lb113oCIiDRLiwJLVlYWVVVVRERE1Ls9IiKCtLS0E2rI5s2bCQgIwNvbm3vvvZf58+czcODARo+fOXMmwcHBrktMTMwJvf4pzScIbv/ETHsuzoI3L4dN77u7VW2iT5cAvnzwHO4Y1xOAiwZGEBXsQ3ZROR+uO8Sc7/Ywf8MhAMornfzpi+2ux2YXlbNo+9FDmY11OiYfKaG8OuB8sC65td+KiIgcQ7vp0+7fvz+JiYmsWrWK++67jylTprBt27ZGj58xYwZ5eXmuS3Lyaf4BElQ9HDTgcqgqh4/vhsV/7nD7EzXFw2Hn5jHdAfi/+Vt47pudTH9/I+n5pXy8/hD7sooIC/Dm1jPNMe+vrf252Zicy4PvbqDf77/mNx9uPOq592YUuq4v25XJ4bySk/xuRESkRosCS1hYGA6Hg/T0+t3h6enpjRbUNpeXlxd9+vRh5MiRzJw5k6FDh/L3v/+90eO9vb1ds4pqLqc9L3+44S0Y97D5ftmz8NGdUHF6fbDeOKY7no7aBfYsC77Zmsb8DWaTxbvO7snU6h6ZJTsz+G5nBi8u2cs1//qBzzamUlFl8cmGVErK6w8p7cmsDSxOCz5en9IG70ZERKCFgcXLy4uRI0eyaNEi121Op5NFixYds97keDidTsrKylr1OU8Ldjtc9CRc9S+zSu7W+fCfq6GswN0tazPhgd7886bh/DKhHw9e0AeAt1ceZPWBHACuHBZN7/AAxvXpjNOCqa+v4S8LduC04PK4KCKDfCivcrJqfzaH80p4fuEu8oorXD0svcL8AdM7U9mCGhgRETl+LR4Smj59Oq+88gpvvvkm27dv57777qOoqIipU6cCcPvttzNjxgzX8eXl5SQmJpKYmEh5eTkpKSkkJiayZ88e1zEzZsxg2bJlHDhwgM2bNzNjxgyWLFnCLbfc0gpv8TQ1/Ba4/VPwCYbklfDODXDkIDirew2cVfD9X+GL6VBR6t62ngQTB0fxUEJfrh9lapt2pRdiWTAmNpSuIb4A/POmEdw5rie+ng48HTb+fM1gXrh5BOf1N/safb87ixkfb+Yfi3YzZ8keVw/Lvef1ppOfJwezi/nv6iTXa1qWRUZBaaM1MCIicvxavDT/5MmTyczM5LHHHiMtLY1hw4axYMECVyFuUlIS9jrLxqempjJ8+HDX97NmzWLWrFmMHz+eJUuWAJCRkcHtt9/O4cOHCQ4OJi4ujm+++YaLLrroBN/eaS52HNxW3cOS9CP8PQ48fCHuerPR4vbPzHFBUXDuI+5s6UkTE+rHkK7BbE4xy/dfMSzadV+ovxePXTGQhxL6UlZRRZcgHwDO6RvOvDXJfLYxlcwC08u3aHu663pct2CmX9yfP3yyhb/+bxdXxEXTyd+Lv/5vFy98t4dhMSE8nNCX8/p3aXF7SyuqsNnA28Nxom9dRKRD0eaHp4NDa+HzhyFzOzgr69xhAywTYqatgZCOOdPqX0v28OyCnXjYbaz+vwRC/b2OefyRonJG/Glhg7sf2G2w7amJeNhtXP7P5exIK+Ca4V2ZOi6Wq+f8gLPOY96YOrpFoaWorJKLnl9KsJ8XX/zibBz2jr3RpYgInKR1WOQU1W0U3Lccfp8BU7+GfhMhpAfc9jH0GAeVJfD5g1Dwk6npHSPLMmlEN7qH+nHrmT2aDCsAnfy9iKteQRegk5+n63pMqB8+ng48HHYev2IQAPM3pHDDSytwWnDxwAguGWwK0F/5fl+L2rn6QA6peaVsP5zPmup6GxERMbRb8+nE7oAeZ5lLjYAIeOlc2LsYZsdBVBzYHJCXDEVZMPFpGP0z97W5FUQE+bDsN+e36DHn9A1n46E8+nQJ4JrhXXnum50A9A4PcB0ztndn/n7jMB75YBOlFU6CfDz40zWDKa908s3WNH7Yk82u9AL6RQQ26zVX7st2Xf9q82HO7NW5RW0WEenI1MNyuosYZIpzY+KhqgwOrTFFuvkp5vsvfwVfPQL/Ggv/GH50L0wHNXVcLNcO78qz18VxwYDaYZ0+XQLqHXfVsK68/bN4zukbxuwbh9El0Idunfy4eKDpZXnjxwPNfs2V+2p7Vb7anEaVs2P0cImItAbVsIhhWXA4EfJTzcJzQV1h26dmZ+i6Bk+C615zSxPdxbIsxs5cTFp+KX+ZNITJo7s3+ZiV+7K58eWV+HjaWfjL8cSE+gFmtd3DeSV06+RXr0aloLSCYU8tpMpp4evpoKSiinn3nKleFhHp8Jr7+a0hITFsNrNHUXTtjC66jTaL0W37FPpNgB//CVs+grgboc+FZojpNGCz2fjdZWfwWWIqlwyJatZj4nuGMqJ7COuTcrn7P2t5dOIAXly6l8SkXMqrnPx8fC9mXFK78eLag0eoclp0D/VjTM9QPlx3iC83nfiwUGWVk4oqC1+v0+PfSkQ6LvWwSPMtmAEr/1X7few5cOHjEDPafF9eZIaMOvd2T/vamdTcEq58YTlZheVH3RcW4MWq3yW4ellmfr2dl5bu44ZR3bh0SBR3vL4GL4edj+8/i4yCUj7ZkMqvL+5P985+LWrDdS/+yIHsYr568GzXtG0RkfZEs4Sk9Z3/O+hSZ0PKA9/Dqwnw1jXw/fPw92HwzxGw+E8dZobRiYgO8WXurSPxcthx2G3cPrYH304fT7CvJ1mF5a6ZQFmFZXxbvfvzmb06M75fOAlnRFBe5eT211Zz15tr+WxjKk99sRWAj9cf4oXFu3E6LfKKK/jZm2v45XuJLN+dVW/RuuScYtYePEJWYRmv/rC/7U+AiEgr0pCQNJ93INz3I1SWQsFhs1Ju4n/NDKO9i2uPW/acqYU5/3cQ3M197W0HRsWGsnD6uTjsNrp1Mr0jCWdE8NH6QyzYksaRonIe/WgT+aWV+Hk5OLtPGDabjb9eP5RL//E9Kbm1+0B9uz2DeauTmDF/M5YF/SIC2XY4n2+rd5yevyGFm8bEMPPaOABW7K2ddfTOyiTuP68Pwb6eiIicijQkJCcmZz+s+bdZnO6MK8DD28wqwgKbHQKjobwABlwBlz9v7j/NfbstnZ/9Zy3Bvp4Ul1dSUWUxKDqIp68ZwtCYENdxW1Ly+OMX27g8Loof9mSzYGv9GVpDY0JIOVJMVmE55/YLZ/nuTJwWPHddHNePiuHheRv4JDHVdfwVQ6Px93KQcEYECQMjjmqX02lhs5manYyCUv785XbO79+Fq4d3PWnnQkSkuZ/fCizS+vYsguV/M0NGdfU63xTyZu+B8/8Pwvq4p31uVlpRxcg/LqSoejfoy+Ki+PvkYXg4Gh+hTUzO5eo5PwDQ2d+LgtJKyqs3XowKNuvMzF2yl78u3IWPp53Ppp3Nrf9eRUZBGdeP7MYH6w65nsvfy8HS35xPJz8vdqUXMCAykPIqJ7e9upp9mYVMO78Pb69KYk9GISF+nqz5vwQ8j9E2EZEToVlC4j59LjSXIwfN4nMFqfDxz2Hfd+YCkLMXfrYYHKffj6CPp4OEgRF8mpjK8O4h/PX6occMKwDDYkIY3y+cpbsyeeqqwSzblcl7a5MBuH1sLJ4OO/ef34fVB3L4fncWU19fQ0ZBGd4edp64chDF5VVkFpSRUVDKgexi/vq/nRw6UsL3u7O4elg04YHerN5vamqe+Hyb63VziytYsTebc/uFn7wTIiLSDOphkbaRvAa++R2EdIc930JprullCR8Adg+zXYD99PkrPiO/lE8TU7l+VDdC/JreLgDMXkNp+aX0Dg9gT0YhE2Yvw9fTwfJHz3c9R0Z+KRfPXkZucQUA4/p05p2fnel6jh/3ZnHzK6safY0bR8fw2cZUOvl5MSAykEU7Mpg8Koa/XBdX77gH3lnP1tQ8/nj1YM7pqzAjIsdPQ0LSfm14Gz59oP5tsefARU+arQIqy8wU6S5ngENFoo1Zn3QEX08HZ0TV/3n/YlMq0/67AYBHJvTngfPrD73d+cYaFu/IwMth5+5ze/LS0n1UOi1XMCkpNztGr086ws2vrDpqWCirsIxRf/rW9Xw/O7sn/3fZGdhs2qxRRFpOQ0LSfg27xSxGt/t/0CkWCjNNvcsrF9Q/bvB1cN2rZv2XzR/ALR9C9DB3tLhdGtG9U4O3Xx4Xzap9OXy+KZXLGljo7skrB+HpsHHTmO6c178L4/qEsXp/Dvec2wvAtchcfM/OhAV4kVVYXm9YaG31dOyaFXn/vXw/Xh52fjNxQL3XSc8vZVtqPuf1D8dms1FRZfZY+jQxlYQzujRrxWARkRrqYRH3qCw3tS0hPSBnH3z9G0hNhNI88PCBiiKwnGZV3U3zzGPCz4CfL62daWRZZoVeaZBlWSfc6/F/8zfzzqokhncP4T93jiHQx5M/frGNV5fv55b47sR1C+bRjzYD8MerBnHb2FjArLA78e/fsyejkLm3jmR8v3Amvfgj2w7nA2C3wbx7xjKmZ+gJtU9ETn1aOE7aNw8v07tis5mVcW/9CH6zFx7Lgt8dgnMfMcfVhBWbHTK3w5KZJqhs+xT+2h8+uAOqKtz1Ltq11hiimTquJ4E+HmxIyuXWV1eTX1rhWvBuTM9QJo/uzq8v7gfAswt2UlBq/i3mrUlmT0YhAO+uTuKzjSlsO5xPoI8Hw7uH4LTg4Xkb2JKSx5Gio1cCFhH5KQUWaZ/O+TVEDjHXu4+FSa+a68v/Bn8bDO/fDoXpsHU+fPaL+ivrZu6Ct66FuWebXhs5bn26BPDu3WcS4ufJxuRc/m/+Framml6S0bGmd+T+8/rQO9yfgrJK3luTTGFZJbO/3eV6ju93Z/Likr0A/OKCPrx1Vzyxnf1IzSvl8n8uZ+SfFrJ4R3qbvadDR4o58+lF/PGLbQ3e73RalFc626w9ItI8CizSPnl4wY3vwvhH4Ya3YPC1cPZ0M1yUX72myOBJYHPAxnfhhdHwwVT4z9Xw4lmwdxGkbYZXLzZFvg3J3gvr32pfPTSFGVCc4+5W1DO4azAv3zYKgM83plLltOga4kt0iC8AdruNu88x9S+vLd/Pox9uIquwnNjOfoyo7k05kF2Mt4ed60fGEODtwYu3jmRkj04EenvgtOC7HZlt9n4+2ZBCWn4pb/54gMyCsqPuv/nfKxn/3HcUlVW2WZtEpGkKLNJ+hcSY5f0DqqfNJjwOj+yB69+AO76C616Dq14Auydk74atH5t1XpwV0Pdi6HcJVJWZGUlb55vnKC82X8sKTbj5bBp8/Wjta5YVwOI/w97vjr/dJUdg//fgrGrZ44pzYE48vHxe+wpRmOGfK4ZGu74fHVu/4Pfq4V0JC/AiNa+ULzcfxmG38dgVA7lpTG1h7RVDo+nkb6ZfnxEVxEf3ncVTVw8CYGdawQm3cXd6AW+vPMhz3+wgKbu40eMWVm9lUOm0mL/hEOWVTtYnmd2y80srWLkvh8N5pa6eJBFpHzRLSE4t3oEw6Jra74fdbMJJ8mrI2gUBXSCsH3QdaYaJFjwKq182C9etehmSfoSBV4FvKOQlmedY+6qpp+l5rgk36VvArzNM3370VgLlRVBVDr4Nz9ChqhLevBLSNkHEEJj4tHneunL2QVA304tkWeCsNNO3dy2Akhxz2bcU+ia02mlrDb+9ZAALt6VRWuFk9E+KZX08HUwd15PnvtlJeKA3L9w0nPhenSkqq+Spz7dRUFbJbWf2OOo5+0eYArud6QWuImGn0+KHvVlEBPnQLyKwWW1buC2dn7+1Fmf1yOCu9EJeuX3UUcdlFJSyMTnX9f28Ncl8tyOTFfuyeXZSHH0iAlz37c4oUFGwSDuiwCKnPv8wGHApcGn92202mPgM5B2CnV+ZsAKmYLfGGVfA9s9h4R/qP7Y4G7Z9BnHXmxDi8DAzm169GDJ3wFm/gHN/A15mQ0OKssE3xOyrlLbJ3Ja+2YSXq16A4bea2759EpY/bxbQG3oTbPkI8g/D1C9h+xe1r79tvgksTid8Pws2vQdX/hN6nNVaZ63Fuob48pdJcXyzNY0r6/S21LhvfG8GRgUR1y2YzgEm6Pl7e/D2z+LJKSqvt09SjV7h/jjsNvJKKsgoKONgdjGPfbqFHWkFhPp7sep3Fza5LUBOUTkzPt6E0zI9N9sP57N8dxalFVX4eDrqHfvdDtO70rdLAIeOlLAvs4h9mUUArNiXXe/Y3emFR71WYnIuLy7Zw2NXDKJr9ZCYiLQNBRbp2OwOmPRv+O5p8AuFLgPh84ehMA2GXA/XvgKLnjLrvBRlQvQICO8P6143PS8Z22DVSzBxplnQLn2Led7lfzO1MQOvhtT1kLIOOvU0QQfgwsdNj8/Gd+HTaaZeprIUVv7L3J+bBEv/UtvOb35vnqPG9i/g4j+bIavtn5vbFvwW7llaO5U7aw94+pgdsZ1VZsfsmHjwOca0/vIi8PA97lWFrxrWlauGNbwZot1u4/wBXY66vaGgUsPH00FsZz/2Zhax+VAe099PJL/U1I7kFJWzLTW/0cf/d1US3+/OJDW3hKzCcvp2CWD+/Wdx3nNLSMsvZdX+HMZXrx2TmJzLnoxCPttoNoO8Ymg0B7OL+Wh97R5LW1PzCA+s7VGrmeVU198W7mLprky6hvjx2BUDG31fcpraOA8ytpue3/D+7m5Nh6PAIh2flz9M+HPt911HmhqVgVeaD/+Ex82lRn4qrP8PJK0wF4Avp4NX9XDB8Ntg3xLIS4Y1r9Q+7sh+8zV6OIx7yEzF9vI3vS7Ln6897sLHzX0HvjebQS59Fg4uN/cFdzfBpijDzHLKSwaHlzn+8EazsWTfBNj4Hnxyr2nTvd+b1/jxn2a464b/NHwe0reZxfmihsLN88xO21s/Bv8u0G2UmY1VE4aKsmDhY9DrPIi74ejnqqo0Q2M1PUwnoH9kIHszi3jjxwPkl1YSEeRNzzB/Vu7LYe3BIw0GlvT8Un7/yWbXEJDDbuP5G4bh4+ng/AHhvLs6mcXb0xnfL5xd6QVMfmkFZXVm/lx4Rhf8vTzIKixjwqBIfjd/M3syCgn1r90mYXdG/bqayiqna9G8mqndv5u/mZQjJbx028ijenOSc4p5b00y94zvRZCPVmzu8Epy4ZP7waqCH2ab3xNX/lNrRbUiBRY5/QR0gaGTG78/KNoMMdX0bHQZaHpaSnMhtBdc/jdz+55FsPNLEzLiroedX8OB5SaQ2Ks/vC6dZbYYOPij6X0ZeDWMmmruO/th8zU3GRKrZzKdcbkJAmv+bcKKTzDc9J5py8o5sORpOLSmunfGgrJ8eOcG05sDZhgrazeE9T36fS2ZCZUlkLwS5p5rZltZdabvjn/UFDlXlsG8W8xxG981PTh1h6KcVfBqAhw5ADe/DzFjmn3qG9IvIpCvNqexfE8WAAlnRBAd4svKfTmsO5jDXWf3POoxH69PwWlB/4hAJg6OZFj3EIZ0Cwbg/P5dTGDZmcGMiioefHcDZZVOwgK8OVJczpCuwQyMCsJms/HmnWOwLIvnF+4iq7DMtQEkQHp+GXklFazen8OAyEByispdO2xvTc1jS0oe/11l6qC+3Z7O5XH1h8l+8+EmVuzLxsNh4+GEfid0jqQZcvaZovrIwe55/QPLTVjx8DXF/hveggGXQf9L3NOeltg4zww7J6+GM++DC37v7hY1SIFFpCHnPgJpW2D0XTDqTlO7kr4FEp6o3d+o/0RzqXHmfeZSl80Go39mLo2+1q9MMLCqzC84h7cJLIFRcOvHEDEQOvUwvTkp62qHjoZUh6SsneZ7h7f5RfnDbOhzkekFCuoK3UaaPZq2fwbYzNBYTcFx/0tN782OL0wIyjtkepiSV5r7LSd8dDfc8UXtQn/bP4dUs1cRb11jtkzoMfa4T/WAyPqFtQkDI/D3Mr+a1hw4ctSKvZZl8UH1TtV3nd2TG0bH1Hv8uD5heHnYSc4p4YaXVrAjrYDO/l58/dA5BPp44O1hr/d8NpuNQdFBLN2V6eqxCfT2oKCskjnf7eHlZfvoHe7P9aNqX8dpwcyvt7u+/2RDar3AsiMt31UTs+7gkRafk+zCMsqrnEQFq06mWZxOeP1SszbT1K+h+5lNP6a17V9qvg6/xUwOWP43+Ob/oPeFpsC+vUpNhPk/r/1+1cumPq8dtlmBRaQhUUPhocTa7+9cANl7zHBPawvtBde8ZEJEj3EmFNyz1BTm+lXPUgmKhvN+C2vfgIhB0O9iGDnV1NF8Ns308lzxN3h7krntp2vP1AxnDbwKLnzMDFH1u8T06AD88A9TeJz4jvne5jDTx7993Pzl+o9h4BcGlz4LP/zdHOMTYnqd/nsD/Ozb4x6zrzsTyM/LwdhenQHwdNjILCgjOaeE7p1rh57WHTzCvqwi/LwcXBp39F5J/t4enNmrM8t2ZbLpUB4edhuzrh9arz6lnqJsbnIsZiX9KcOLUH8vBkUH8f3uLF5bbob59mYW8cqyfQB4OeyUVzn5YU9tke7SXRkcKSp3Tdt+88eDAHhQyYakXKqcFg67CUmlFVV42G14OOw4nRYbknOJ6xbsKi6uclpMevFHsovKWf7oBQT7Hns4qaS8Cqdl4e99Gv86z0+BgsPm+sf3wL3Lj13LdTLsqw4sPcdD7/NhwzuQsxdWvWiGiFuqrbYe2fyB+drzXMjYYYajD/5g3kM7o3VYRJrDO/DkhJUacdfDOb+q/QUVPaw2rNQ451fwy82m/mTUnebYEbfBlM9NoOqTAN2rh27snjDqLoibDJ5+UF5dQHruI2YrhKvm1IYVgHEPmnVtRtwO4x6GO740NT43vGXCm80BxVnw4Z2mlsbTD+77wbxeWT68e5NZf6ZGRQl8N9MMLeWnHv1+K0ph5wIozadHZ3+8PMyvovH9wvHxdODj6WBIVzPEU1MvAqZ35d/fmxBx2ZAoArw94PAmE6rqeOTi/lwWF8WjEwewcPr4+sXA5UWw6I/m9SvL4Z1JTNw/k7sdXwJmdd++XUyIqqzpcgGyq7cQmPyTHp2YUF8qqiy+3Gw+MPOKK/hkQwp3Ob5il/cUnnT+g30HzEq/xeWVnPPsd0yaa2qjPklMYdKLP/KHT7a4nm/tgRwOZBdTUFrJ3syjC3/rsiyLyS+v4LxZS1zbIpyWsnfXXs89CF883DZrGW35yCxauemD6p5OG8SebX5fXPiYOWbxn8xQS0usfgWe7dn4opetxemsXaNqzM9re4x3fn1yX/c4ncaRXKSDqLvOyzUvwtrXTFCJMIuyUZBmZjp16nHs8f3Bk8ylrsjB8PNlpq7lm9+ZoSqAkXeY2pYb/gOvnG/+knxtoukFKsoys6FqQkRhBlz/Onz7hPm+z0Vm2CpjG0TG4bjjS87vUoiVtpWLzpjqeulRsaGsT8pl8Y4MLh8ahZfDzh8/28SCrWnYbHBLfHdY8hdT1+PpZ4Kbpx+sfZUhI6cy5+YRDb/P//3enCOAHme7hrcucazmhapr6NslgL511mMZFhPClpQ8Kp0WAd4e3DEulrdWmh6UEd1DmDg4kqe/2sFLy/aSklvC5xtTsVcU8kuf+dixmORYTtm758O9S9heGEpmQZnrsmKv6aX5cN0hfnFhX7pmfI/n/97icnsPFjuHk5pbwojunahyWizbncmPe7IY2aMTFw2MxGG3kZpXyqZDeQBsS84ivneX2vqp00lWdWAJ7WVqq7Z8BAXpppewZuHJ42FZMO9mUzcWPsAsTzD0xtr7N7xt6sc+rh7yjYqr/UNj+K1mR/rtn8F7t8E9SyDo6B7Bozir4Pu/mj8APn3ADHOdPf3Ee1tKcs3SC3UlrzK9U95B5g8eu8NMONj5FVzyl3ZXMKzAItKRdIqFi56qf1tgZP1ZUMfDw9sUEIf0MNOnz55ubg8Ihxv/C/+50qxP88EddV43yhRBHloNfx9mViCG2i5oMGvW/PtC5h7Zh82rEmvjSuj5IoT2ZExsKC8v28eXmw+zbud+fsdrPGpbRbDH5fSc8BDDfpxmam8AKorh7WtNsKosNd3z96806+fUtfvb2rACtbOzgEH2g3SzZXJ52WZ6HMrHxlgs7Dx4YR8+S0zlk8RURsV2oleYPxFB3qTnlzFxcCRXDu3KrP/tIjmnxLVn0oN+3xPgLCLXO5q0EgcDKpJh2Sz2xfyOUPKpwIPth/PZUb3Cb6XT4p0lm3hkx88YUZrHCC84ZIWxOPMDMgpCue7FFSTlFAMWr3xvI7azH2/dFc+WFBNW4m3biftgGpanB5sir2Fv79uIH9jn9Fkrpqbo/Iwrzcy7+T83/7Zzx8EV/zDF5ll7IP7nLRsq2rPIfHiDWfYgeTUMuBy8qwNt1u76x/ccX3vdZoOrXzRDyRnbzEzDm95t+jX3LjbDW3YPs6jkoqdMb9F5v234+IoS067YcxpfrmDrfPN/c8DlcNlfTYFt+rbaYbQBl5slEnqdZ4qG85JN2Art1XABv5sosIhI89hsZuho3IP1b4+Kg1+sN7Utm96H0J7QbyKMnGJmR717owkr4WeY7vLtn0PMaBgxBd6fAlm7sAHYPbElraiul+nMBXE38uAFU1i1ZiXPlz9JV5vpjXjIYz5897n5ZW73NGvkJP7XrIdjGmqGCBLfNhto7l9mwlP2HtPVDhB/r+mNWf48jL7brJ1xcDm/9ZjH2B1muGZiQCSHgkcwvl8XBkYFY7fbmHpWT2w2G7+6qD9fbTnM9SNj6OTvxVcPns2SnZnsTD3C0C4Obt7wLeRDWtx9/GY5fOb9B9j8Pl7Fg/je+ykyrRC+SZ5PTnoSf/D4jC+r4vFb/zE2ex6pViieVNHNlkXs7jf4IfQ3JOUUc5n3Rv7q+AfPWbfxavZ5/Pv7ffh5e3CN/Xue9XwZz/IqKIehe1+ibPcSxn32GE9cMYg7xtWZZWVZJiTuXQxdqmuh2oP8w7D4j3Dm/cc3y6cmOIT1M0OdYYtNr0bWTni3zozA3INmIcfm+rG6XmvozSYA5SaZYvYzLjdDi3mm+BvfULNCdd+fnE/vALjudbO/2c6vTJDOSzYBY/yjENzAmkY1w0Cjf2aK5hf+wczwO3LA/MwWZ5vXHnazqUl753qzRMLVL5rbwPTS7PsOgmNMbVnNjMcdX8COLwGr/mvW9Kx6+kLvC8zsx/9WL2dw7b/NkHXOfjPD0su/+eevldksy7KaPqz9y8/PJzg4mLy8PIKC2rjYSkQat/lD80ERf9/R67YcWG62Thh2q/lL7tNp9Xo9uPxvWGtfx5a2iYrgWGwjbsPj+1nmL+aIwaYWJ3qYGYb69nGIHGrC0Te/M4XG5UUc9cs5cgjctdD8ci45YrZZWPEv+GZGvcOqRt9D5YRn8PY4xhDLobVmSKlTrPlrfMNbtfVC/uHk/XwDQ59extuef+Zsx9Z6D33F72f0K1jNeMcmyvGgwnLgbyvjvvKHwGbjRc/ZlNj9eW3UZ8xZdpAVgY8SXJFJQXA/hqQ/QWxnP/oGV/JCyk142yr40Wc8GV0TmLjnKXxsFdxY/nvsDk9eHbwF3/N/ZYq4376udsVnuydMW2MCZnP+GQ/l0T3Uj2C/OkXAZYVmyKJz76MfUFVh/kqPPafpXo0vppuFGnuMg6lfNas99fz1DChIhbu+NWEYTO/e/35vnjekuwkb2ExBbnNCUWoivDze1G89tBFWzDEFtMNvM6Hn8EZ46VxTjH73YtOL0tgU5q8eMT/n3kGm5gtMyLnmpfqhsTgH/trfLG3w82Wmfuz752HRkw0/b5+LYM9Cc71mDabk1aY3J20zBETCL7eaPwLyksErEMoLzPkYeLUprg2IhBverJ39uOdbmHermSFYVQZh/c1aT69cYH62J1f/IdCKmvv5rcAiIu1LWaH5cFjydO1tvp3MEE9gpOnaP5xouv8bmnpZWQYvjKr+gMJMK3VWgKe/WX+n/6VH7xF15AD8fWj1NzbAgsBo88u+sW72omyYPQQqio6+z+4Jlz4Ho6Zy4V+XEJm9kne8ZgKQYwUQaiukzPLE21a/MHS7szuXlj/NxIGRPLRnKgPsySzqdD0HMwu402OB67hzyv9JsrMzt3p8x588XmGnsxu3eP6NXuGBXHFoFrd5fMsuj/5EVCQTbCs2H6qRg03vgIePOZ8Fhznc40o8xtxF+Nq/wtm/NH9dN2BD0hGu+dePRAZ48OYl3vQfNs58kL8+0dR33PYJ9Bpf/0GfPwTr3jA9aVf+o+FzCKY34K8DzOwUML11DQUgMEWiNlv92oqyApjZzVx/9MDR+3yV5psi2A/ugG2fQK/z4bb5TddnfHgXbPnQLB8w6d9mscm3rjYLLf5qp1l08aO7TOH5nU0UqRZlwz+GQ1keYDMB98h+s6TADW/VFsAv+J1ZbyliCNxXJ7hv+cj0VvqGmn3O0jbVzuir4RtqgtXfBtWGIjA9PB9ONa/18BZI32r+rX76f6Aup9MEm+cHma89x5tp236d4YHVZjuUVtTcz2/NEhKR9sU7AM79tVmRuMZlz5uwAhDWB4Zc1/g6ER7ecM3LZtr2ze/DbR+bgtyb55mNMxv6Rd0pFqKGmesJT5i/hAtSIWVt/eM2vG3W5ElaCavmmrDi38X8Fdonwayb8/sM+EOma4HAMT1D+cE5mM2dLmKj1ZtLy2ZywBnhCiuLu0yBcQ9jBUaTdtYTPJTQn/su6MucyqsAuPDIB7Vhxc98UEwJM2vvXGH/HoD5VWeTVVTBhuQjvFx1GZbNTr/KnQTbiqmybGaG174lWA5vcq770FVLEXHgc4I+uN4Mm331SPUHVbHZSqLu207KxZtynix7lv6fX8WRV64y21ckrzJ/iX/7uBluqrFzgQkrYIpOqyob/rcCcy5rwgoc/UEMpnD769/C09Hw2S/q35e9x3z179LwpqQ+QdUrWj9hVo3e953peTnWbuoFaSbcAIydZr72GGd6KIoy4PCG2rqZ5tR4+Hc2oS1qGNz4DjywyuwlZjnNzLsN75hd4lfOMcef88v6jx88ydSeXPB/cOa9cGWd/cmGXG/CeEmOKdYtyzc/z30nmPuXPGO+RgwyQ1D9Lj52WAET0n2CYdQd5vuaNWYuebbVw0pLKLCISPtjd5gx+eAYs97M4Gtb9vgeY01A6Teh+Y+57jWY9Cqc9WDt49a9Abv+Z4auls82szaSV5mZI6tfMsdc+hxMWw23fgR9LjQfBnX+eo/v2RmwMbXgXq4q+yM5HuHMdZiZJgecEaTF3Q8XPYntV9s5f+IkHk7oR/dQP750nsm/Kq8k2TJTsgv6XAVnmQ/Pizw20JVM4u07cGJjVcCFAFRUWeR5dzXd/UChPYgryv/MNiuWKhz8svznnDevmPSAM0gMOh+7zcIbM12b7D04N39oek3+OdIMcQFk7SZs+5u87fU0ExwmwHVK+8EMO9RI3VC9MCFmeO6zabX3lRypHYaqqzCTnOwsVn5ZXQQdVF3PkfhubZjI3mt6amYPMcMxlSVm2G3XN7XP46pfaSI4hPY0oQVgxQvw1tWUb19AVWUDYWrdm6ZGKibeDDmCCch9qnugdn1TJ7A0cxXjQVfDz5eaxSE9vE3o6H+pGXb59H5Y9qw5LuHJo2fr/ZTdbh4/ba3ZD61mobwV1YFnyA21u9rXLCwZE9+8dtYVf58p/gXof1nT7TrJVHQrIu1TeH/45Zamj2stnXvXDkUMvMrMZkp8p+Gu95pNLjv3MTt+H0N8LzPNNauwDIDYzn4k+V/ClAM+7HR244VuR28YGezriY+XJ8+W38izlTfiZytjw+Qr4chu+PYJYvLW8jMPM1PlQMBwQjrHQl4mAMO6d8J20VMm9MVNJfg7B5fv+xOdKCCbYKio5P01ySwomMQzzv2sd/ZlYI8oRqf8h6r592KnOiws+K0paP7vZK50VoIdyux+vFg+kQcd87HbLDPVt/+lpnh50R/NcMvnD5kZNV0GmsuWD81mnnWn3+cfxjnnTHzKShlo2cEGy3v/irN3/NH0bP3wd/Pv/9HPzAwwICM4jtU5flzuWEnVF7/CMe1sUwDakp6OsQ+YnrpP7of9y/Dav4xNXsOIm7GkNmRWVdTOJBtzT/3H97vE7Pa+5SPTWwPNDyw/5fAwIfl/fzCrV5cXmXWVzvpF048F096a99zzHNi7qHYm3pDrju5tOp7AEtzVLNO/ZxFc/rzbpzkrsIiI/FSfBDM9Nnuv+cu/otgUHJ55n/kr8+XzTGg5+5dNrnsSFexL91C/6mnJ0CssgK6dfHl1n6mZ6f+TrQnAbBcQFezD3kxTH9O5Uwjenh4mIIR0x56bxFQP08uQ0/sa+nkH8t1OE1iGx4RASAxM+jcBwJs9nfzpy22s2JvNed1C+Gj9If61ZC8lFaFcgakT6p9ewmfWu3jbKnBaNqq8AvDM2mVmoFhONtOX7yvP4PzJD/LOZ3mkF3fisdBv8b3iH9BlgFm7I3s3zIk3gcPuWb168yETWHZ8SalvFwpTtmG/5C+kLXqZgWW5+AHYoMDy5c4Vnfli6GT67Zxbv8i0xzi44Pf8+Ucf/pe+n2H2PXTLTzabfZ7325b3dAyeBJFxpC+eQ/DWt4krT6R429f4DbrU3L/tU7Obu38XUydV1xmXw1eBtcNQcGLTfj194bJZx//4GrF1wmDEkNpVp6OH126jcbx7fp39S3NpBzQkJCLyU56+ZruBR/eb4scH18Ovd5lf3CHd4c5v4Oq5MOyWZj1dfM/aVYt7hfszMMoUFsaE+hLYyE7O0XXWUOkZVr3uh81mptgCpd6d+V+nmxh4yc/p06V2obvh3UPqPY+Xh52nrhrMwunjeeqqQfh7OSipML0oFw+MwMfTzs5CX96qSgBgVuUNPFlSPRXYclIZNYLrSv+PZytvJKbfcMb17sy7VRcyZ/AHlEePJrnYk4qbPgD/cBNWwNRaRMWZ5d09/SH/ED5L/0jYno/44G8P4bVlHgDfOs6hPGwQP3T7GeV48lDaJViXPGseA6Zg9/ZPocdZbDyURwk+/LXietO0LR+bupmaD+SwFmwNEdaX1f1/w5tVZoaOc1nNhqbfmh4iMIsj/rROyjvQ7BVUw+Ftfh7cLWqoqa8BGFJn2KbPReZrQIRZQ+kUp8AiItJSYX1h2E3N7iKPr94fCaBnmD8XD4rgggFd+MUFjf91XnfRt15hdda+GP8beGgjPo/u5uKH5uLn411vP6ZhMSGNPqe/t0e9TRqvHt6VCwdEAPAP+20cnrKClV2n8N/K80gJPRPC+rN9/IuU4UVkkA8B3h6c1ccUXS7dlcmt/17FOc9+R/9/HeZBv79QET0aBl9n6oAAPH0p7WU+NPMt835+5viKPvZUKuzenPXwf/Ca9iNjb30cbw8729OL2Nx1MvxiHdzxFVzxd3B4cqSonAPZpodqvc+ZVFgObFk7zdBMbhJ4+vFuRjdmfbMTZ53tFI4lJbeEVysvpczyICB9tVkT6L+TTU9az/GN7/9Td5ioc+/2sbKww8Ps/t51lFkioMawm8waRCPvcPtwTmtQYBEROcnq97AEEOjjyWt3jOaGOjtA/1TdnZpj62z+iN1hZoHU+aAcFB3ExQMjuOOsWEL8jr3L7g2jzRRgLw875/YL555zexHb2Y8/XjuMqJ4DObdfOE7sPNflGZi2mh2FJiz17mK+jqsOLJtT8lhdvc+T04LPkn2YXPkkJVe+XK9tT1TdySMV93Cj71zKouNx2Eyg8Bx0FX6B5rwE+3oycbCZBfb+2mSzhH3sONeHbOKhXHPuwvy5euxAVjkHmCf/+lEAqvpO4A9f7uOF7/bw4bpDx3z/NVKOlJBBJz6uOsfcsO0TU2gbN9nsQO4d0PADO/euXSCuHa0Cy7m/hrsX1d+KILQX/GoHnP8797WrFSmwiIicZDGhfpzTN6zecFBTokN8XNd7hjfy4VnNw2Hn5dtH8cSVg5p83pE9Qnl2Uhwv3TqSAG8PhsaEsOSR87lqmJmlU9NDk5icC+Cqo+ld3YauIb70rO7xsdngtTtG8cUvzibY15P1Sbn86oNEapb3yi+t4OMdxXxQdR4zbzkP74l1to2oWZW1Wk14+zQxldKK+lOON1a3ZWhMCOf0DWORs3qfqOIsADK7X+raqPLZb3aQ34yNIFNySwB4ofJqDnr1NbNgpnxham8amDKfX1pR266EJ80+VD8typWTSoFFRKQNvHVXPIt/dR6+Xs0bQmh0SKgV3DA6pv4O1nXUBJYD2cXkFpezJ8Os3Fu3TuayIWYTv19f3J8LBkQwuGsw/54yCk+Hja82p7GkugD4682HKa900rdLAHHdgs3024QnzUyYuvvuAGN7daZriC8FpZUs3JZe776a8DQsJoTBXYNZxqjaOz392RlwpuvbrMJy/vHtT/b4aUBqdWBJIZypXrPgpv9Cz3NIyStlznd7KCmvDU0ZBaWc99wSJr+80oSxiIEw9Uuz1QTw7+/3cf3cH8krOY13zG4DCiwiIu1QTKgZBvLxtNcrwD3ZQvy8XD0oicm57Ms0gaV3nV6ehxP68v1vzueB8/u4bhsdG8rU6n2L/vTlNiqqnHy8PgUwtTK2mhqKsx+Gi/901ArCdruN8weY4Yxd6QWu2y3LqtfD4uPpILhrX3Y4q4fT+k9kX54JF1HBplfqtR/288OerGO+z5QjJa7rB3OKKa90AvDEZ1t57pud/Hd1kuv+j9enkFNUzsbkXA5W19LUbd/cpftYc+BIk68pJ0aBRUSkHYoJ9ePxKwby/A3DcNjbtmByaLdgANYcyOFg9XTsuoHFw2F3Baq6pl3Qh1B/L/ZmFvHwvERW7Tc1LlcPb2CTvwbU1O0czit13XboSAlHiivwdNg4I8oUF4/s0YmXKy8jzzMcxj7AgSwzbHXVsK7cMKobTgsefHcDh/NKjn4RIK+kgoIys2Ccj6edKqfFgewiyiudrtCxM80sb29ZFh+sTXY99se92fWe63BeqWuNneSc+mHGHeZvOMSbPx5wdzNOCgUWEZF2auq4nlxaPfzSlmqGhV75fj9VTotQfy8igppYzh0I8vFk+kVmPZQvNx8G4MxeofWGt46lpockrU5g2Zlmelv6dAl0bUQ5skcoHzvP5Qa/16DrSNcMotjOfjx11WAGRgWRXVTOjS+vJDE5l6KySjILylzPWTMcFOrvRf/qGVZ7MgpZezCH4uqhoP3VIWh9Uq6rjgfgh731e1E2VRcEAyQfaZvAcjivxNUjVFeV0+LRDzfz+GdbXT1jHYkWjhMRkXqGdTerpNZ8KD511aDaIZ0m3BLfnRA/T9bszyGrqJx7z21kI8MGRFYHlro9Izurh4f6RdT28IzsYdq3K6OAvJIKDmabQNGjsz8+ng7m3jqSm15ZycHsYq751w+ubY5euX0UFw2McA0HRYf40LtLABsP5bEno9AVVgD2VYeUD9eZ3pXe4f7szSxi5d5snE4Le3Wv18ZDea7HJOWY592Skkegjwc9Ordu7VHNc1/+z+VcNSyav984vN59+SUVlFeZf7NV+3Po1USx9qlGPSwiIlLPGVGB+Hqa3oxHJvSvt3ZLU2w2G5fHRfPkVYOZc/MIhlQPLzVH3SGhmplGu12BpXatmfBAb3p09sOyYM3+HA5VB5DYMDNM1b2zH189eA6Xx0XV25PxnVUHgdoZQl1DfF3FxLvSC1i6K9N1bHZROTlF5XyxyfQUPX7FIHw9HWQXlbtCFMDmOoElOaeYw3klXPuvH7n5lVWu9wBQVFbJ8t1ZzZrBdCybU8zrLdqeQdVP1pzJrVP0u2pf/aGrjkCBRURE6vH2cDD3tpE8e10c95/X/B6SExUZZHpYisurXDUmO9PN0EbdwAIwJtas4fL2qoNUOi28PexEBNZOBQ/28+SFm0ewcsaFfPrAOACW784ip6jcNSQUHeLLkK4mUH25+TDbD+djs0Ggjxl8+HJTKgWllQT6eHB2nzBGV6+nU1PHYllWvSGhlCMlrD1whPIqJym5JfUKdGd+vZ1bX13FmD9/yyMfbDxq6nZz1QyXFZZVsv1wfr37jhSXu66v2p9TLzB1BMcVWObMmUNsbCw+Pj7Ex8ezevXqRo/dunUrkyZNIjY2FpvNxuzZs486ZubMmYwePZrAwEC6dOnC1Vdfzc6dO4+naSIi0grG9wvnhlExzR4Kag2+Xg5C/MxWBWl5pVRWOdlbXYvR/yeB5bI4U9tTM4W6e6ifa5imrshgH4bGhDAoOohKp8WCLWkcqtPDcnafMG4f28PVEzM4OpjB0SbEvFddbDuyRyfsdhvjepsVi7/fbV7zQHYx+aWVeHnY8XTYKK9ysnhHhuu1N1aHGcuyWLzd3F5a4eSDdYdc7W6pjILa+p411Qv31citE1gO55WSnNNw0fGpqsWB5b333mP69Ok8/vjjrF+/nqFDhzJhwgQyMjIaPL64uJhevXrxzDPPEBkZ2eAxS5cu5YEHHmDlypUsXLiQiooKLr74YoqKiho8XkREOqaaXpbDeaWu6ca+ng66dapfuHt2nzDCAmoLgZuqF7liqBnW+mJTqquGpVsnX2w2G09eOYhfX9wPL4edG0Z1o1e4ea4tKaYHY3R1b84F1WvXLN+dRVZhmat3ZWBUkGvq+bd11pCpWT/m0JESUvNK8bDbXEFrQ/KRFpyVWun5tcXDRweW+sNNK/d3rGGhFgeW559/nrvvvpupU6cycOBA5s6di5+fH6+99lqDx48ePZrnnnuOG2+8EW/vhqvMFyxYwB133MGgQYMYOnQob7zxBklJSaxbt66lzRMRkVNYpGumUImrfqVvRMBRvSceDjtXD6utram3fUEDaha7W7Evm22pJojUhAybzca0C/qy9akJ3DY29qhi1VHVRb59IwIZGhNCpdPikw0prK6etj20WzDdq6d51wxlQW1gWVldTxLXLZhz+5qtDWrWlmmpujOoVu8/Um/Y58hPAsuqffUDzamuRYGlvLycdevWkZCQUPsEdjsJCQmsWLGi1RqVl2eKikJDQxs9pqysjPz8/HoXERE5tUUF1/aw7ExruH6lxjUjatd36dHEasAxoX6M7dUZy4LyKicB3h6uBfJqeDrMR2JND4u5zcbQOhtK3jDK7MX07+/3894aM2Q0vn94g+vSbE3Np7zS6Qo28b06MyzGhJ/Nh/KorHLy4pK9/GfFgaMKaBtTd0goq7DMNaUbaoeEasLbqtO5hyUrK4uqqioiIiLq3R4REUFaWlqrNMjpdPLwww8zbtw4Bg8e3OhxM2fOJDg42HWJiWl8EzERETk1RAaZXo/0/FJ2ZRw9pbmugVFBrqLZmq/H8vLtI3n7rnjeumsM//vluQT6eDZ4XO+w2tcb0jUYH8/a7RSuGBqNt4edtPxSKp0Wlw6J5Pz+XYjpVBtY+kUEEOzrSXmlk51pBa4F9OJ7htKnSwD+Xg6Kyqv49/L9/GXBDh77dCu3vbqqXhhpSEWVk6xCE0pqQtWa/bW9KDVDQhcMiMBuM0NR6fnHfs5TSbubJfTAAw+wZcsW5s2bd8zjZsyYQV5enuuSnJx8zONFRKT9q9vDsivt6CnNddlsNl6dMop37z7TtdjdsQT6eHJ23zDO6Rt+zO0OunbyxcvDfDzW1K/UCPLx5JLqnaXDArz509VDsNlsriEhgMFdg129Ml9vOUxSTjF2myneddhtxHUz9/29zp5HP+7N5q431h6zpyWjevE7T4eNCYNMG9YerA0sNbOEunXypX+k2WRz/cGja2WqnBZllbWzlL7dlt7kENXG5Fy375XUosASFhaGw+EgPb3+xlTp6emNFtS2xLRp0/jiiy/47rvv6Nat2zGP9fb2JigoqN5FRERObTU1LFtT810zhAZENv77vUuQD2OrZ++0Fofd5urVie91dGnCwwn9uGhgBC/eOoJQf7Ozc0xobQAaFB3MsOr1Z15dvh8wIaamR6cmzJRUT22ee+tIgnw82JySx1srDjTarpreki6BPgytDj01hcFQ28PSyd/TVXeztk5gKa2o4u2VBznnL4sZ+cdvOXSkmANZRfzsP2uZ/PIK1xYHP+V0Wkx9Yw3Dn/ofW1LyGjymLbQosHh5eTFy5EgWLVrkus3pdLJo0SLGjh173I2wLItp06Yxf/58Fi9eTM+ePY/7uURE5NRV08OSWVCG0zJL+9eEmLb0l0lxPH3NEM7vf/Su1rFh/rxy+6h6vS/1eliigxjXxxTXllWvFlzTIwLU6w0aExvKxMGR/GbiAABm/W8Xv/1oE7e/tpr/rkqqv2t0dWCJCPJmcFcT4nalF7h6S3JLTA9LiK+XazXgdQePUFxeyZ+/3MaZMxfx+0+2kJpXSmFZJT/syXItRFda4eS3H2/C2UAPz460AnKKyvHxdDTa29UWWrw0//Tp05kyZQqjRo1izJgxzJ49m6KiIqZOnQrA7bffTteuXZk5cyZgCnW3bdvmup6SkkJiYiIBAQH06WN2+nzggQf473//y6effkpgYKCrHiY4OBhf37bbpVRERNzrp+HkjrNi3dKOQdHBDIpu/iq9wb6eDIgMJKuwjCHdgvHz8uDDe8dSXF5FRJBPvTqc4d1DXNdvijf1lzeP6c4H6w6xMTmXedXFvMt2ZfJ/n2zGw25jaLcQJlYPRUUE+dA1xJdOfp4cKa5gZ1oBcd1COFJkelhC/DxdK/huTc3jdx9v5pPEVACig30ID/JhY3IuO9IKCPCujQEr9+Xw4tK93H9e73rr79RsCDmmZ6hrqMwdWhxYJk+eTGZmJo899hhpaWkMGzaMBQsWuApxk5KSsNfZNjw1NZXhw2v3O5g1axazZs1i/PjxLFmyBIAXX3wRgPPOO6/ea73++uvccccdLW2iiIicogJ9PAnw9qCwrJLoYB8Szoho+kHtgM1m49Np46issvDzMh+to2IbnukaEeTD5XFRZBSUcclgM93abrfxtxuG8pcFO+ge6keInxfz1iSRnFNCRZXF2oNHqKju/YgI8sFmszG4azDf785iS0o+cd1CXLOEOvl50a2TL+GB3mQWlLnCyuzJw7hiaDQfrzfBaMfhAteqvgMiA9mRVsBz3+xk5b5snr9hGOGBZimSmg0fz67uNXKX49r8cNq0aUybNq3B+2pCSI3Y2NgmlwfuaMsHi4jI8YsK9mF3RiG3nNkDD0e7mxvSKG8PB97N/FR94eYRR93WKzyAl24b5fr+vvG9ySos46kvtvHFpsOuwtiI6sX1BkWbwLI5JY/ySidF1cNHIX6e2Gw2RnbvxIKtZsTi4oERXD3cTAM/I8oMJ+1IyyfI19TV/OHygWxOyeP5hbv4fncWv/5gI2/eOYbySqdrPZezers3sJw6PwkiInJamH5RP64f2Y0pbhoOai/sdhtdgny4aGD9XqbIYNPzUTOVe2tqnqt+xW4zM5kARsV2ct1WUyMD0KdLAHabWWiuZr+j/pGB3Du+N5/cPw5Ph42luzJZsjODxORcSiqq6OzvxYBI99WvgAKLiIi0M5cMieK564fWq684nZ3TN5y6WzrVbPJYU3i743ABmdVTnoN9PV2rAl86JIrYzn48dGE/V00LgI+no96ieZ39vVzbHAyMDuL2sbEA/PnL7Xy2MQWAs/qENbhXU1vST4OIiEg7FurvRVy3kNohoerC5O6hfgT6eFBQWulaQK6Tn5frcdEhvix55PwGn3NAVBB7M8005p/O/Hnwgr58tP4QuzMK2Z1hppaPa+Wp48dDPSwiIiLt3Pi+tfUjNTUsNpvNtbP0st2mMDbYr+HVe39qQJ2Q0v8nQz3Bfp786erB9Ar3J7p6t+uaGUrupB4WERGRdm58/y78Y/Eegn096w2VjezRiRX7slleHVjq9rAcy4Co2sX4Glpb5fK4aC6Piz7qdndSD4uIiEg7N6J7CI9fMZDnrourd/u5/cIBs6EjmBlCzVG3gLZ/ZMN7NbU36mERERFp52w2G1PHHb0K/PDuIa51a8CsctscXUN86R8RSF5JhWuac3unwCIiInKK8nTYOat3Z/63zezx16mZPSx2u42P7z8Lp1W70F17pyEhERGRU9g51cNCACH+zethAfD39nBtyHgqUGARERE5hY3vWyew+J46AaSlFFhEREROYd07+7kWhuvWqeNuGHxqDFyJiIhIo+beOoKtqfkM797J3U05aRRYRERETnF9ugTSp4t79/o52TQkJCIiIu2eAouIiIi0ewosIiIi0u4psIiIiEi7p8AiIiIi7Z4Ci4iIiLR7CiwiIiLS7imwiIiISLunwCIiIiLtngKLiIiItHsKLCIiItLuKbCIiIhIu6fAIiIiIu1eh9mt2bIsAPLz893cEhEREWmums/tms/xxnSYwFJQUABATEyMm1siIiIiLVVQUEBwcHCj99uspiLNKcLpdJKamkpgYCA2m63Vnjc/P5+YmBiSk5MJCgpqteeVhul8tx2d67al8922dL7bzomea8uyKCgoIDo6Gru98UqVDtPDYrfb6dat20l7/qCgIP3QtyGd77ajc922dL7bls532zmRc32snpUaKroVERGRdk+BRURERNo9BZYmeHt78/jjj+Pt7e3uppwWdL7bjs5129L5bls6322nrc51hym6FRERkY5LPSwiIiLS7imwiIiISLunwCIiIiLtngKLiIiItHsKLE2YM2cOsbGx+Pj4EB8fz+rVq93dpFPeE088gc1mq3cZMGCA6/7S0lIeeOABOnfuTEBAAJMmTSI9Pd2NLT61LFu2jCuuuILo6GhsNhuffPJJvfsty+Kxxx4jKioKX19fEhIS2L17d71jcnJyuOWWWwgKCiIkJIS77rqLwsLCNnwXp4amzvUdd9xx1M/6xIkT6x2jc918M2fOZPTo0QQGBtKlSxeuvvpqdu7cWe+Y5vz+SEpK4rLLLsPPz48uXbrwyCOPUFlZ2ZZvpd1rzrk+77zzjvr5vvfee+sd05rnWoHlGN577z2mT5/O448/zvr16xk6dCgTJkwgIyPD3U075Q0aNIjDhw+7LsuXL3fd98tf/pLPP/+cDz74gKVLl5Kamsq1117rxtaeWoqKihg6dChz5sxp8P5nn32Wf/zjH8ydO5dVq1bh7+/PhAkTKC0tdR1zyy23sHXrVhYuXMgXX3zBsmXLuOeee9rqLZwymjrXABMnTqz3s/7uu+/Wu1/nuvmWLl3KAw88wMqVK1m4cCEVFRVcfPHFFBUVuY5p6vdHVVUVl112GeXl5fz444+8+eabvPHGGzz22GPueEvtVnPONcDdd99d7+f72Wefdd3X6ufakkaNGTPGeuCBB1zfV1VVWdHR0dbMmTPd2KpT3+OPP24NHTq0wftyc3MtT09P64MPPnDdtn37dguwVqxY0UYt7DgAa/78+a7vnU6nFRkZaT333HOu23Jzcy1vb2/r3XfftSzLsrZt22YB1po1a1zHfP3115bNZrNSUlLarO2nmp+ea8uyrClTplhXXXVVo4/RuT4xGRkZFmAtXbrUsqzm/f746quvLLvdbqWlpbmOefHFF62goCCrrKysbd/AKeSn59qyLGv8+PHWQw891OhjWvtcq4elEeXl5axbt46EhATXbXa7nYSEBFasWOHGlnUMu3fvJjo6ml69enHLLbeQlJQEwLp166ioqKh33gcMGED37t113lvB/v37SUtLq3d+g4ODiY+Pd53fFStWEBISwqhRo1zHJCQkYLfbWbVqVZu3+VS3ZMkSunTpQv/+/bnvvvvIzs523adzfWLy8vIACA0NBZr3+2PFihUMGTKEiIgI1zETJkwgPz+frVu3tmHrTy0/Pdc13nnnHcLCwhg8eDAzZsyguLjYdV9rn+sOs/lha8vKyqKqqqreiQaIiIhgx44dbmpVxxAfH88bb7xB//79OXz4ME8++STnnHMOW7ZsIS0tDS8vL0JCQuo9JiIigrS0NPc0uAOpOYcN/VzX3JeWlkaXLl3q3e/h4UFoaKj+DVpo4sSJXHvttfTs2ZO9e/fyu9/9jksuuYQVK1bgcDh0rk+A0+nk4YcfZty4cQwePBigWb8/0tLSGvz5r7lPjtbQuQa4+eab6dGjB9HR0WzatIlHH32UnTt38vHHHwOtf64VWKTNXXLJJa7rcXFxxMfH06NHD95//318fX3d2DKR1nXjjTe6rg8ZMoS4uDh69+7NkiVLuPDCC93YslPfAw88wJYtW+rVv8nJ0di5rltrNWTIEKKiorjwwgvZu3cvvXv3bvV2aEioEWFhYTgcjqOqy9PT04mMjHRTqzqmkJAQ+vXrx549e4iMjKS8vJzc3Nx6x+i8t46ac3isn+vIyMijCssrKyvJycnRv8EJ6tWrF2FhYezZswfQuT5e06ZN44svvuC7776jW7durtub8/sjMjKywZ//mvukvsbOdUPi4+MB6v18t+a5VmBphJeXFyNHjmTRokWu25xOJ4sWLWLs2LFubFnHU1hYyN69e4mKimLkyJF4enrWO+87d+4kKSlJ570V9OzZk8jIyHrnNz8/n1WrVrnO79ixY8nNzWXdunWuYxYvXozT6XT9QpLjc+jQIbKzs4mKigJ0rlvKsiymTZvG/PnzWbx4MT179qx3f3N+f4wdO5bNmzfXC4oLFy4kKCiIgQMHts0bOQU0da4bkpiYCFDv57tVz3WLy3RPI/PmzbO8vb2tN954w9q2bZt1zz33WCEhIfUqnqXlfvWrX1lLliyx9u/fb/3www9WQkKCFRYWZmVkZFiWZVn33nuv1b17d2vx4sXW2rVrrbFjx1pjx451c6tPHQUFBdaGDRusDRs2WID1/PPPWxs2bLAOHjxoWZZlPfPMM1ZISIj16aefWps2bbKuuuoqq2fPnlZJSYnrOSZOnGgNHz7cWrVqlbV8+XKrb9++1k033eSut9RuHetcFxQUWL/+9a+tFStWWPv377e+/fZba8SIEVbfvn2t0tJS13PoXDfffffdZwUHB1tLliyxDh8+7LoUFxe7jmnq90dlZaU1ePBg6+KLL7YSExOtBQsWWOHh4daMGTPc8ZbarabO9Z49e6ynnnrKWrt2rbV//37r008/tXr16mWde+65rudo7XOtwNKEf/7zn1b37t0tLy8va8yYMdbKlSvd3aRT3uTJk62oqCjLy8vL6tq1qzV58mRrz549rvtLSkqs+++/3+rUqZPl5+dnXXPNNdbhw4fd2OJTy3fffWcBR12mTJliWZaZ2vyHP/zBioiIsLy9va0LL7zQ2rlzZ73nyM7Otm666SYrICDACgoKsqZOnWoVFBS44d20b8c618XFxdbFF19shYeHW56enlaPHj2su++++6g/eHSum6+hcw1Yr7/+uuuY5vz+OHDggHXJJZdYvr6+VlhYmPWrX/3KqqioaON30741da6TkpKsc8891woNDbW8vb2tPn36WI888oiVl5dX73la81zbqhsmIiIi0m6phkVERETaPQUWERERafcUWERERKTdU2ARERGRdk+BRURERNo9BRYRERFp9xRYREREpN1TYBEREZF2T4FFRERE2j0FFhEREWn3FFhERESk3VNgERERkXbv/wEe63fxDX8SUgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 0s 2ms/step\n",
      "\n",
      "NNRegressor:\n",
      "MeanSquaredError: 46451.769531\n",
      "RootMeanSquaredError: 215.526733\n",
      "MeanAbsoluteError: 64.556648\n",
      "FitPercent: 9.976524\n",
      "CosineSimilarity: 0.552459\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'MeanSquaredError': 46451.77,\n 'RootMeanSquaredError': 215.52673,\n 'MeanAbsoluteError': 64.55665,\n 'FitPercent': 9.97652411,\n 'CosineSimilarity': 0.552459}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NNRegressor()\n",
    "model.train_and_test(training_data=df_train,\n",
    "                     features=features,\n",
    "                     testing_data=df_test,\n",
    "                     targets=targets,\n",
    "                     denorm=denorm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
